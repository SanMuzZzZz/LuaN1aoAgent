# core/executor.py
import asyncio
import json
import os
import time
import tempfile
from typing import Dict, Any

import httpx
from rich.errors import MarkupError
from rich.panel import Panel

from core.console import sanitize_for_rich


def _get_console():
    """Lazy initialization of console to avoid circular imports."""
    from core.console import console_proxy
    return console_proxy
from core.events import broker
from core.graph_manager import GraphManager
from core.prompts import PromptManager
from llm.llm_client import LLMClient
from tools.mcp_client import call_mcp_tool_async
from conf.config import (
    EXECUTOR_MAX_STEPS,
    EXECUTOR_MESSAGE_COMPRESS_THRESHOLD,
    EXECUTOR_TOKEN_COMPRESS_THRESHOLD,
    EXECUTOR_NO_ARTIFACTS_PATIENCE,
    EXECUTOR_FAILURE_THRESHOLD,
    EXECUTOR_RECENT_MESSAGES_KEEP,
    EXECUTOR_MIN_COMPRESS_MESSAGES,
    EXECUTOR_COMPRESS_INTERVAL,
    EXECUTOR_COMPRESS_INTERVAL_MSG_THRESHOLD,
    EXECUTOR_TOOL_TIMEOUT,
    EXECUTOR_MAX_OUTPUT_LENGTH,
)


async def _execute_with_retry(func, *args, max_retries: int = 3, delay: int = 5, **kwargs):
    """
    æ‰§è¡Œå¼‚æ­¥å‡½æ•°é‡è¯•æœºåˆ¶ã€‚

    åœ¨å‘ç”Ÿç‰¹å®šç¬æ—¶ç½‘ç»œé”™è¯¯ï¼ˆè¶…æ—¶ã€è¿æ¥é”™è¯¯ã€JSONè§£æé”™è¯¯ï¼‰æ—¶è¿›è¡Œé‡è¯•ï¼Œ
    æœ€å¤šé‡è¯•max_retriesæ¬¡ï¼Œæ¯æ¬¡é—´éš”delayç§’ã€‚

    Args:
        func: è¦æ‰§è¡Œçš„å¼‚æ­¥å‡½æ•°
        *args: å‡½æ•°çš„ä½ç½®å‚æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
        delay: é‡è¯•é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º5
        **kwargs: å‡½æ•°çš„å…³é”®å­—å‚æ•°

    Returns:
        å‡½æ•°æ‰§è¡ŒæˆåŠŸçš„è¿”å›å€¼

    Raises:
        Exception: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åçš„æœ€åä¸€ä¸ªå¼‚å¸¸
    """
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except (httpx.ReadTimeout, httpx.ConnectError, json.JSONDecodeError, Exception) as e:
            if attempt < max_retries - 1:
                _get_console().print(
                    Panel(
                        f"å‘ç”Ÿç¬æ—¶é”™è¯¯: {type(e).__name__} - {e}ã€‚å°†åœ¨{delay}ç§’åé‡è¯•... (å°è¯•{attempt + 2}/{max_retries})",
                        title="è­¦å‘Š",
                        style="yellow",
                    )
                )
                await asyncio.sleep(delay)
            else:
                _get_console().print(
                    Panel(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚æ”¾å¼ƒæ‰§è¡Œã€‚é”™è¯¯: {type(e).__name__} - {e}", title="é”™è¯¯", style="bold red")
                )
                raise


async def _check_halt_signal(
    graph_manager: "GraphManager", subtask_id: str, last_step_ids: list, messages: list, cycle_metrics: dict, log_dir: str = None
) -> tuple[bool, tuple]:
    """
    æ£€æŸ¥ç»ˆæ­¢ä¿¡å·ã€‚

    Args:
        graph_manager: å›¾ç®¡ç†å™¨å®ä¾‹
        subtask_id: å­ä»»åŠ¡ID
        last_step_ids: æœ€åçš„æ­¥éª¤IDåˆ—è¡¨
        messages: æ¶ˆæ¯åˆ—è¡¨
        cycle_metrics: å‘¨æœŸæŒ‡æ ‡
        log_dir: æ—¥å¿—ç›®å½•

    Returns:
        tuple: (is_halted, return_value)
    """
    halt_file = os.path.join(tempfile.gettempdir(), f"{graph_manager.task_id}.halt")
    if os.path.exists(halt_file):
        _get_console().print(Panel("ğŸš© æ£€æµ‹åˆ°ç»ˆæ­¢ä¿¡å·ï¼ä»»åŠ¡å·²ç”±å…¶ä»–ç»„ä»¶å®Œæˆæˆ–ç»ˆæ­¢ã€‚", style="bold yellow"))
        try:
            await broker.emit(
                "execution.halt", {"subtask_id": subtask_id}, op_id=os.path.basename(log_dir) if log_dir else None
            )
        except Exception:
            pass
        for step_id in last_step_ids:
            if graph_manager.graph.has_node(step_id):
                graph_manager.update_node(step_id, {"status": "aborted"})
        graph_manager.update_subtask_conversation_history(subtask_id, messages)
        return True, (subtask_id, "aborted_by_halt_signal", cycle_metrics)
    return False, None


async def _compress_context_if_needed(
    messages: list,
    executed_steps_count: int,
    llm: "LLMClient",
    graph_manager: "GraphManager",
    subtask_id: str,
    log_dir: str = None,
    output_mode: str = "default",
    update_metrics_func: callable = None,
) -> list:
    """
    æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥ã€‚

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        executed_steps_count: å·²æ‰§è¡Œæ­¥éª¤æ•°
        llm: LLMå®¢æˆ·ç«¯
        graph_manager: å›¾ç®¡ç†å™¨
        subtask_id: å­ä»»åŠ¡ID
        log_dir: æ—¥å¿—ç›®å½•

    Returns:
        å‹ç¼©åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    from rich.panel import Panel

    should_compress = False
    compression_reason = ""

    # ç­–ç•¥1: æ¶ˆæ¯æ•°é‡é˜ˆå€¼
    if len(messages) > EXECUTOR_MESSAGE_COMPRESS_THRESHOLD:
        should_compress = True
        compression_reason = f"æ¶ˆæ¯æ•°é‡è¿‡å¤š ({len(messages)} > {EXECUTOR_MESSAGE_COMPRESS_THRESHOLD})"

    # ç­–ç•¥2: æ‰§è¡Œè½®æ¬¡é˜ˆå€¼
    elif (
        executed_steps_count > 0
        and executed_steps_count % EXECUTOR_COMPRESS_INTERVAL == 0
        and len(messages) > EXECUTOR_COMPRESS_INTERVAL_MSG_THRESHOLD
    ):
        should_compress = True
        compression_reason = f"å®šæœŸå‹ç¼© (ç¬¬{executed_steps_count}è½®)"

    # ç­–ç•¥3: ä¼°ç®—tokenè¶…é™
    else:
        total_chars = sum(len(msg.get("content", "")) for msg in messages)
        estimated_tokens = total_chars // 4
        if estimated_tokens > EXECUTOR_TOKEN_COMPRESS_THRESHOLD:
            should_compress = True
            compression_reason = f"ä¼°ç®—tokenè¶…é™ ({estimated_tokens} > {EXECUTOR_TOKEN_COMPRESS_THRESHOLD})"

    if should_compress:
        try:
            if output_mode in ["default", "debug"]:
                _get_console().print(Panel(f"ğŸ§  è§¦å‘æ™ºèƒ½å‹ç¼©: {compression_reason}", style="blue"))

            system_prompt_msg = messages[0] if messages and messages[0]["role"] == "system" else {"role": "system", "content": ""}

            recent_messages = (
                messages[-EXECUTOR_RECENT_MESSAGES_KEEP:] if len(messages) > EXECUTOR_RECENT_MESSAGES_KEEP else messages.copy()
            )

            messages_to_compress = []
            if len(messages) > EXECUTOR_RECENT_MESSAGES_KEEP:
                messages_to_compress = messages[1:-EXECUTOR_RECENT_MESSAGES_KEEP]
            else:
                messages_to_compress = messages[1:] if len(messages) > 1 else []

            if messages_to_compress and len(messages_to_compress) >= EXECUTOR_MIN_COMPRESS_MESSAGES:
                compressed_summary, compress_metrics = await llm.summarize_conversation(messages_to_compress)

                if update_metrics_func and compress_metrics:
                    update_metrics_func(compress_metrics)

                if compressed_summary:
                    compressed_message = {
                        "role": "system",
                        "content": f"ğŸ“Š æ™ºèƒ½ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆå‹ç¼©è‡ª{len(messages_to_compress)}æ¡å†å²æ¶ˆæ¯ï¼‰:\n\n{compressed_summary}",
                    }

                    messages = [system_prompt_msg, compressed_message]
                    messages.extend(recent_messages)

                    graph_manager.update_subtask_conversation_history(subtask_id, messages)

                    if output_mode in ["default", "debug"]:
                        _get_console().print(
                            Panel(
                                f"âœ… æ™ºèƒ½å‹ç¼©å®Œæˆ: {len(messages_to_compress)}æ¡å†å² â†’ 1æ¡æ‘˜è¦ + {len(recent_messages)}æ¡è¿‘æœŸæ¶ˆæ¯",
                                style="green",
                            )
                        )
                else:
                    if output_mode in ["default", "debug"]:
                        _get_console().print(Panel("âš ï¸ å‹ç¼©æ‘˜è¦ä¸ºç©ºï¼Œä¿æŒåŸå§‹æ¶ˆæ¯å†å²", style="yellow"))
            else:
                if output_mode in ["default", "debug"]:
                    _get_console().print(Panel("âš ï¸ æ— éœ€å‹ç¼©ï¼šå†å²æ¶ˆæ¯ä¸è¶³æˆ–å·²æ˜¯æœ€ä¼˜çŠ¶æ€", style="yellow"))

        except Exception as e:
            _get_console().print(Panel(f"âŒ ä¸Šä¸‹æ–‡å‹ç¼©å¤±è´¥: {e}", style="red"))
            if log_dir:
                error_log_path = os.path.join(log_dir, "compression_errors.log")
                try:
                    with open(error_log_path, "a", encoding="utf-8") as f:
                        f.write(f"[{time.time()}] Compression error: {e}\n")
                except Exception:
                    pass

    return messages


async def _call_llm_and_parse_response(
    llm: "LLMClient",
    messages: list,
    update_cycle_metrics_func: callable,
    subtask_id: str,
    console_output_path: str = None,
    output_mode: str = "default", # Add this parameter
) -> tuple[dict, list]:
    """
    è°ƒç”¨LLMå¹¶è§£æå“åº”ã€‚

    Returns:
        tuple: (llm_reply_json, updated_messages) æˆ–å¼•å‘é€€å‡º
    """
    llm_reply_json, call_metrics = None, None
    try:
        llm_reply_json, call_metrics = await _execute_with_retry(llm.send_message, messages, role="executor")
        update_cycle_metrics_func(call_metrics)
    except httpx.ReadTimeout:
        _get_console().print("LLMè°ƒç”¨è¶…æ—¶ï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œã€‚", style="red")
        if console_output_path:
            try:
                with open(console_output_path, "a", encoding="utf-8") as f:
                    f.write(f"[ERROR] LLMè°ƒç”¨è¶…æ—¶ï¼Œå­ä»»åŠ¡ {subtask_id} ç»ˆæ­¢ã€‚\n")
            except Exception:
                pass
        raise RuntimeError("llm_timeout")
    except Exception as e:
        _get_console().print(f"LLMè¾“å‡ºæˆ–è§£æå¤±è´¥: {e}", style="red")
        if console_output_path:
            try:
                with open(console_output_path, "a", encoding="utf-8") as f:
                    f.write(f"[ERROR] LLMè¾“å‡ºæˆ–è§£æå¤±è´¥: {type(e).__name__}: {e}\n")
            except Exception:
                pass
        raise RuntimeError("llm_parse_error")

    if not llm_reply_json:
        _get_console().print("LLMè¾“å‡ºè§£æå¤±è´¥ï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œã€‚", style="red")
        if console_output_path:
            try:
                with open(console_output_path, "a", encoding="utf-8") as f:
                    f.write(f"[ERROR] LLMè¾“å‡ºè§£æå¤±è´¥ï¼Œå­ä»»åŠ¡ {subtask_id} ç»ˆæ­¢ã€‚\n")
            except Exception:
                pass
        raise RuntimeError("llm_empty_response")

    messages.append({"role": "assistant", "content": json.dumps(llm_reply_json, ensure_ascii=False)})

    # æ‰“å°LLMå“åº”
    json_str = json.dumps(llm_reply_json, indent=2, ensure_ascii=False)
    safe_json_str = sanitize_for_rich(json_str)

    # åªæœ‰åœ¨ default æˆ– debug æ¨¡å¼ä¸‹æ‰æ‰“å° LLM æ€è€ƒè¿‡ç¨‹
    if output_mode in ["default", "debug"]:
        try:
            _get_console().print(Panel(safe_json_str, title="LLMæ€è€ƒ (ç»“æ„åŒ–)", style="cyan"))
        except MarkupError:
            _get_console().print(f"LLMæ€è€ƒ (ç»“æ„åŒ– - åŸå§‹):\n{safe_json_str}")

    return llm_reply_json, messages


def _update_previous_steps_status(
    llm_reply_json: dict,
    last_step_ids: list,
    graph_manager: "GraphManager",
) -> None:
    """
    ä½¿ç”¨LLMçš„åˆ¤æ–­æ›´æ–°ä¸Šä¸€æ­¥çš„çŠ¶æ€ã€‚
    """
    raw_prev_status = llm_reply_json.get("previous_steps_status", {})
    if isinstance(raw_prev_status, str):
        try:
            parsed_prev = json.loads(raw_prev_status)
            previous_steps_status = parsed_prev if isinstance(parsed_prev, dict) else {}
        except Exception:
            previous_steps_status = {}
    elif isinstance(raw_prev_status, dict):
        previous_steps_status = raw_prev_status
    else:
        previous_steps_status = {}

    if last_step_ids:
        for step_id in last_step_ids:
            status = previous_steps_status.get(step_id)
            # Normalize 'executed' to 'completed' for frontend compatibility
            if status == "executed":
                status = "completed"
            
            if status in ["completed", "failed"]:
                graph_manager.update_node(step_id, {"status": status})
            else:
                # If LLM returns something else or nothing, keep the status set by tool execution (usually 'completed' or 'failed')
                pass


def _check_failure_patterns_and_trigger_reflection(
    llm_reply_json: dict,
    last_step_ids: list,
    graph_manager: "GraphManager",
    failure_counts_per_parent: dict,
    messages: list,
) -> list:
    """
    æ£€æŸ¥å¤±è´¥æ¨¡å¼å¹¶è§¦å‘å¼ºåˆ¶åæ€ã€‚

    Returns:
        æ›´æ–°åçš„messagesåˆ—è¡¨
    """
    # 2.1. æ›´æ–°å¤±è´¥è®¡æ•°å™¨
    if last_step_ids:
        parent_to_current_steps = {}
        for step_id in last_step_ids:
            parent_id = graph_manager.graph.nodes[step_id].get("parent")
            if parent_id:
                if parent_id not in parent_to_current_steps:
                    parent_to_current_steps[parent_id] = []
                parent_to_current_steps[parent_id].append(step_id)

        for parent_id, current_steps in parent_to_current_steps.items():
            all_failed = all(graph_manager.graph.nodes[step_id].get("status") == "failed" for step_id in current_steps)
            if all_failed:
                failure_counts_per_parent[parent_id] = failure_counts_per_parent.get(parent_id, 0) + 1
                _get_console().print(
                    f"ğŸ” Executor: parent_id '{parent_id}' çš„è¿ç»­å¤±è´¥æ¬¡æ•°è®¡ä¸º {failure_counts_per_parent[parent_id]}",
                    style="dim",
                )
            else:
                if parent_id in failure_counts_per_parent:
                    failure_counts_per_parent[parent_id] = 0

        # 2.2. æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åæ€ (åŸºäºå¤±è´¥è®¡æ•°å™¨)
        for parent_id, count in failure_counts_per_parent.items():
            if count >= EXECUTOR_FAILURE_THRESHOLD:
                forced_reflection_message = (
                    f"âš ï¸ è­¦å‘Šï¼šä½ åœ¨ parent_id '{parent_id}' ä¸‹è¿ç»­ {count} æ¬¡æ‰§è¡Œæ“ä½œå‡å¤±è´¥ã€‚"
                    f"ä½ å¿…é¡»ç«‹å³è°ƒç”¨ 'formulate_hypotheses' å·¥å…·æ¥é‡æ–°å®¡è§†ä½ çš„å‡è®¾å¹¶åˆ¶å®šæ–°ç­–ç•¥ï¼Œ"
                    f"æˆ–è€…åˆ‡æ¢åˆ°ä¸åŒçš„æµ‹è¯•æ–¹å‘ï¼Œä¸è¦å†é‡å¤å½“å‰ç­–ç•¥ã€‚"
                )
                messages.append({"role": "user", "content": forced_reflection_message})
                _get_console().print(f"ğŸ¤– Executor: å‘ LLM å‘é€å¼ºåˆ¶åæ€æŒ‡ä»¤ï¼Œé’ˆå¯¹ parent_id '{parent_id}'ã€‚", style="bold yellow")
                failure_counts_per_parent[parent_id] = 0

    # 2.3. æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åæ€ (åŸºäºçŸ›ç›¾æ£€æµ‹)
    raw_hypothesis_update = llm_reply_json.get("hypothesis_update", {})
    if isinstance(raw_hypothesis_update, str):
        try:
            parsed_hyp = json.loads(raw_hypothesis_update)
            hypothesis_update_data = parsed_hyp if isinstance(parsed_hyp, dict) else {}
        except Exception:
            hypothesis_update_data = {}
    elif isinstance(raw_hypothesis_update, dict):
        hypothesis_update_data = raw_hypothesis_update
    else:
        hypothesis_update_data = {}

    if hypothesis_update_data.get("contradiction_detected"):
        contradiction_message = hypothesis_update_data.get("contradiction_detected")
        forced_reflection_message = (
            f"âš ï¸ è­¦å‘Šï¼šExecutor æ£€æµ‹åˆ°çŸ›ç›¾: {contradiction_message}ã€‚"
            f"ä½ å¿…é¡»ç«‹å³è°ƒç”¨ 'formulate_hypotheses' å·¥å…·æ¥é‡æ–°å®¡è§†ä½ çš„å‡è®¾å¹¶åˆ¶å®šæ–°ç­–ç•¥ï¼Œ"
            f"æˆ–è€…åˆ‡æ¢åˆ°ä¸åŒçš„æµ‹è¯•æ–¹å‘ï¼Œä¸è¦å†é‡å¤å½“å‰ç­–ç•¥ã€‚"
        )
        messages.append({"role": "user", "content": forced_reflection_message})
        _get_console().print("ğŸ¤– Executor: å‘ LLM å‘é€å¼ºåˆ¶åæ€æŒ‡ä»¤ï¼Œé’ˆå¯¹æ£€æµ‹åˆ°çš„çŸ›ç›¾ã€‚", style="bold yellow")

    return messages


async def _build_executor_prompt(
    graph_manager: "GraphManager",
    subtask_id: str,
    main_goal: str,
    global_mission_briefing: str,
    messages: list,
) -> tuple[str, list]:
    """
    æ„å»ºæ‰§è¡Œå™¨æç¤ºè¯ã€‚

    Returns:
        tuple: (system_prompt, updated_messages)
    """
    subtask_data = graph_manager.graph.nodes[subtask_id]
    prompt_context = graph_manager.build_prompt_context(subtask_id)

    manager = PromptManager()
    subtask = {
        "id": subtask_id,
        "description": subtask_data["description"],
        "completion_criteria": prompt_context.get("subtask", {}).get("completion_criteria", "N/A") if prompt_context else "N/A",
    }
    context = {
        "causal_context": prompt_context.get("causal_context", {}) if prompt_context else {},
        "dependencies": prompt_context.get("dependencies", []) if prompt_context else [],
        "causal_graph_summary": prompt_context.get("causal_graph_summary", "å› æœé“¾å›¾è°±ä¸ºç©ºã€‚") if prompt_context else "å› æœé“¾å›¾è°±ä¸ºç©ºã€‚",
        "key_facts": prompt_context.get("key_facts", []) if prompt_context else [],
    }
    system_prompt = manager.build_executor_prompt(
        main_goal=main_goal, subtask=subtask, context=context, global_mission_briefing=global_mission_briefing
    )

    if not messages or messages[0]["role"] != "system":
        messages.insert(0, {"role": "system", "content": system_prompt})
    else:
        messages[0] = {"role": "system", "content": system_prompt}

    return system_prompt, messages


async def run_executor_cycle(
    main_goal: str,
    subtask_id: str,
    llm: LLMClient,
    graph_manager: GraphManager,
    global_mission_briefing: str = "",

    log_dir: str = None,
    save_callback: callable = None,
    output_mode: str = "default",
    max_steps: int = None,
    disable_artifact_check: bool = False,
) -> tuple[str, str, dict]:
    """
    æ‰§è¡Œå™¨å¾ªç¯ï¼šä¸ºå­ä»»åŠ¡æ‰§è¡Œæ€æƒ³æ ‘æ¢ç´¢å¾ªç¯ã€‚

    è¯¥å‡½æ•°å®ç°äº†æ‰§è¡Œå™¨çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©ï¼šåŸºäºæ¶ˆæ¯æ•°é‡ã€è½®æ¬¡å’Œå†…å®¹å¤æ‚åº¦çš„å¤šç»´åº¦å‹ç¼©
    - åŠ¨æ€ç»ˆæ­¢é€»è¾‘ï¼šåŸºäºæ­¥æ•°é™åˆ¶ã€æ— äº§å‡ºç‰©æ£€æµ‹å’Œå¤±è´¥é˜ˆå€¼
    - å·¥å…·è°ƒç”¨æ‰§è¡Œï¼šæ”¯æŒMCPå·¥å…·è°ƒç”¨å’Œç»“æœå¤„ç†
    - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    - è¯¦ç»†çš„æŒ‡æ ‡è¿½è¸ªå’Œæ—¥å¿—è®°å½•

    Args:
        main_goal: ä¸»è¦ç›®æ ‡æè¿°
        subtask_id: å­ä»»åŠ¡ID
        llm: LLMå®¢æˆ·ç«¯å®ä¾‹
        graph_manager: å›¾ç®¡ç†å™¨å®ä¾‹
        global_mission_briefing: å…¨å±€ä»»åŠ¡ç®€æŠ¥ï¼ˆå¯é€‰ï¼‰
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†è¾“å‡ºï¼Œé»˜è®¤ä¸ºFalse
        log_dir: æ—¥å¿—ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        save_callback: ä¿å­˜å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰

    Returns:
        tuple: åŒ…å«ä»¥ä¸‹å…ƒç´ 
            - subtask_id (str): å­ä»»åŠ¡ID
            - status (str): æ‰§è¡Œç»“æœçŠ¶æ€ï¼ˆsuccess/aborted_by_halt_signalç­‰ï¼‰
            - cycle_metrics (dict): æ‰§è¡Œå‘¨æœŸæŒ‡æ ‡å­—å…¸
    """
    from rich.panel import Panel
    from collections import defaultdict

    messages = graph_manager.get_subtask_conversation_history(subtask_id)

    # åˆå§‹åŒ–æœ¬å‘¨æœŸçš„æŒ‡æ ‡
    cycle_metrics = {"prompt_tokens": 0, "completion_tokens": 0, "cost_cny": 0, "tool_calls": defaultdict(int)}

    def update_cycle_metrics(call_metrics: Dict[str, Any]) -> None:
        """
        æ›´æ–°æ‰§è¡Œå‘¨æœŸæŒ‡æ ‡ã€‚

        Args:
            call_metrics: LLMè°ƒç”¨æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«tokenä½¿ç”¨å’Œæˆæœ¬ä¿¡æ¯

        Returns:
            None
        """
        if call_metrics:
            cycle_metrics["prompt_tokens"] += call_metrics.get("prompt_tokens", 0)
            cycle_metrics["completion_tokens"] += call_metrics.get("completion_tokens", 0)
            cycle_metrics["cost_cny"] += call_metrics.get("cost_cny", 0)

    executed_steps_count = 0
    # åˆå§‹åŒ–æ¯æ­¥è¯¦ç»†æ—¥å¿—
    # run_log_path (å·²ç§»é™¤ï¼Œä¸å†ä½¿ç”¨)
    console_output_path = None
    if log_dir:
        console_output_path = os.path.join(log_dir, "console_output.log")
    consecutive_no_new_artifacts = 0
    # ä»å­ä»»åŠ¡èŠ‚ç‚¹è¯»å–æŒä¹…åŒ–çš„æ‰§è¡Œé“¾ï¼Œç¡®ä¿å­ä»»åŠ¡æ¢å¤æ‰§è¡Œæ—¶èƒ½ç»­æ¥ä¸Šä¸€æ¬¡çš„æ‰§è¡Œé“¾
    last_step_ids = graph_manager.get_subtask_last_step_ids(subtask_id)
    failure_counts_per_parent = {}

    while True:
        # æ£€æŸ¥ç»ˆæ­¢ä¿¡å·
        is_halted, halt_result = await _check_halt_signal(graph_manager, subtask_id, last_step_ids, messages, cycle_metrics, log_dir)
        if is_halted:
            return halt_result

        # æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©
        messages = await _compress_context_if_needed(messages, executed_steps_count, llm, graph_manager, subtask_id, log_dir, output_mode=output_mode, update_metrics_func=update_cycle_metrics)

        _get_console().print(
            Panel(f"å­ä»»åŠ¡{subtask_id} - æ¢ç´¢ç¬¬{executed_steps_count + 1}æ­¥", title_align="left", style="green")
        )

        # æ„å»ºæç¤ºè¯
        system_prompt, messages = await _build_executor_prompt(graph_manager, subtask_id, main_goal, global_mission_briefing, messages)

        # è°ƒç”¨LLMå¹¶è§£æå“åº”
        try:
            llm_reply_json, messages = await _call_llm_and_parse_response(
                llm, messages, update_cycle_metrics, subtask_id, console_output_path, output_mode=output_mode
            )
        except RuntimeError as e:
            return (subtask_id, "error", cycle_metrics)

        # æ›´æ–°ä¸Šä¸€æ­¥çŠ¶æ€
        _update_previous_steps_status(llm_reply_json, last_step_ids, graph_manager)

        # æ£€æŸ¥å¤±è´¥æ¨¡å¼å¹¶è§¦å‘åæ€
        messages = _check_failure_patterns_and_trigger_reflection(
            llm_reply_json, last_step_ids, graph_manager, failure_counts_per_parent, messages
        )

        # Store the completion flag from this turn's response, with robust coercion
        raw_complete = llm_reply_json.get("is_subtask_complete", False)
        if isinstance(raw_complete, str):
            is_final_step = raw_complete.strip().lower() in ("true", "yes", "1")
        else:
            is_final_step = bool(raw_complete) is True

        # 3. å¤„ç†äº§å‡ºç‰©æè®®ï¼ˆå¥å£®åŒ–åˆ—è¡¨ä¸å…ƒç´ ç±»å‹ï¼‰
        artifact_proposals_raw = llm_reply_json.get("staged_causal_nodes", [])
        artifact_proposals: list = []
        if isinstance(artifact_proposals_raw, str):
            try:
                loaded = json.loads(artifact_proposals_raw)
                if isinstance(loaded, list):
                    artifact_proposals = [x for x in loaded if isinstance(x, dict)]
                elif isinstance(loaded, dict):
                    artifact_proposals = [loaded]
            except Exception:
                artifact_proposals = []
        elif isinstance(artifact_proposals_raw, list):
            artifact_proposals = [x for x in artifact_proposals_raw if isinstance(x, dict)]
        elif isinstance(artifact_proposals_raw, dict):
            artifact_proposals = [artifact_proposals_raw]
        else:
            artifact_proposals = []

        if artifact_proposals:
            graph_manager.stage_proposed_causal_nodes(subtask_id, artifact_proposals)

        # ... (rest of the logic for processing LLM response, updating graph, etc.)

        # è§„èŒƒåŒ–æ‰§è¡ŒæŒ‡ä»¤ï¼Œé˜²æ­¢å°†å­—ç¬¦ä¸²æˆ–ä¸åˆè§„ç»“æ„å½“ä½œå­—å…¸è®¿é—®
        exec_ops_raw = llm_reply_json.get("execution_operations", [])
        normalized_exec_ops = []

        def _normalize_op_item(item):
            """
            å°†ä¸åŒç±»å‹çš„æ‰§è¡ŒæŒ‡ä»¤è§„æ•´ä¸ºå­—å…¸åˆ—è¡¨ã€‚

            Args:
                item: æ‰§è¡ŒæŒ‡ä»¤é¡¹ï¼Œå¯èƒ½æ˜¯dictã€listæˆ–str

            Returns:
                è§„èŒƒåŒ–åçš„å­—å…¸åˆ—è¡¨
            """
            if isinstance(item, dict):
                return [item]
            if isinstance(item, list):
                return [x for x in item if isinstance(x, dict)]
            if isinstance(item, str):
                s = item.strip()
                try:
                    loaded = json.loads(s)
                    if isinstance(loaded, dict):
                        return [loaded]
                    if isinstance(loaded, list):
                        return [x for x in loaded if isinstance(x, dict)]
                except Exception:
                    pass
                _get_console().print(f"âš ï¸ éç»“æ„åŒ–æ‰§è¡ŒæŒ‡ä»¤ï¼Œå·²å¿½ç•¥: {s[:200]}", style="yellow")
                return []
            return []

        if isinstance(exec_ops_raw, list):
            for v in exec_ops_raw:
                normalized_exec_ops.extend(_normalize_op_item(v))
        else:
            normalized_exec_ops.extend(_normalize_op_item(exec_ops_raw))

        current_step_ops = [
            op
            for op in normalized_exec_ops
            if isinstance(op, dict) and str(op.get("command", "")).upper() == "EXECUTE_NOW"
        ]

        halt_file = os.path.join(tempfile.gettempdir(), f"{graph_manager.task_id}.halt")
        if os.path.exists(halt_file):
            return (subtask_id, "aborted_by_halt_signal", cycle_metrics)

        if not current_step_ops and not is_final_step:
            _get_console().print("LLMåœ¨æœ¬è½®æ€è€ƒä¸­æœªæä¾›å¯æ‰§è¡Œçš„åŠ¨ä½œï¼ˆEXECUTE_NOWï¼‰ï¼Œå­ä»»åŠ¡ç»“æŸã€‚", style="yellow")
            return (subtask_id, "stalled_no_plan", cycle_metrics)

        # Execute tools in parallel
        execution_tasks = []
        potential_parent = last_step_ids[0] if last_step_ids else subtask_id
        current_cycle_step_ids = []

        for i, op in enumerate(current_step_ops):
            step_id = op.get("node_id")
            if not step_id or step_id == "None":
                _get_console().print(f"âš ï¸ è·³è¿‡æ— æ•ˆEXECUTE_NOWæ“ä½œï¼ˆç¼ºå°‘node_idï¼‰: {op}", style="yellow")
                continue
            
            # Ensure step_id is globally unique by prepending subtask_id
            # This is crucial to prevent node_id collisions across different subtasks
            original_step_id = step_id
            step_id = f"{subtask_id}_{original_step_id}"

            current_cycle_step_ids.append(step_id)
            
            parent_id = op.get("parent_id") or potential_parent
            if not graph_manager._is_valid_parent_for_subtask(parent_id, subtask_id):
                parent_id = potential_parent

            hypothesis_update = llm_reply_json.get("hypothesis_update", {})
            if not isinstance(hypothesis_update, dict):
                hypothesis_update = {}
            thought = op.get("thought")

            action = op.get("action") or {}
            if isinstance(action, str):
                try:
                    action = json.loads(action)
                except:
                    action = {"tool": str(action)}
            
            # Add execution step to graph
            graph_manager.add_execution_step(
                step_id, parent_id, thought, action, "in_progress", hypothesis_update=hypothesis_update
            )
            try:
                await broker.emit(
                    "graph.changed",
                    {"reason": "execution_step_added", "step_id": step_id},
                    op_id=os.path.basename(log_dir) if log_dir else None,
                )
            except Exception:
                pass
            
            tool_name = action.get("tool") or action.get("name") or "unknown_tool"
            tool_params = action.get("params") or action.get("arguments") or {}
            
            cycle_metrics["tool_calls"][tool_name] += 1
            
            _get_console().print(
                Panel(
                    f"å‡†å¤‡å¹¶è¡Œæ‰§è¡ŒåŠ¨ä½œ: {tool_name}\nå‚æ•°: {json.dumps(tool_params, ensure_ascii=False)}",
                    title=f"å‡†å¤‡åŠ¨ä½œ{step_id}",
                    style="magenta",
                )
            )
            
            execution_tasks.append(
                asyncio.wait_for(
                    _execute_with_retry(call_mcp_tool_async, tool_name, tool_params), 
                    timeout=EXECUTOR_TOOL_TIMEOUT
                )
            )

        # Real-time metrics update
        if log_dir:
            metrics_path = os.path.join(log_dir, "metrics.json")
            try:
                if os.path.exists(metrics_path):
                    with open(metrics_path, "r", encoding="utf-8") as f:
                        current_metrics = json.load(f)
                else:
                    current_metrics = {}
                
                current_metrics.setdefault("tool_calls", {})
                for tool, count in cycle_metrics["tool_calls"].items():
                    current_metrics["tool_calls"][tool] = current_metrics["tool_calls"].get(tool, 0) + count
                    
                with open(metrics_path, "w", encoding="utf-8") as f:
                    json.dump(current_metrics, f, ensure_ascii=False, indent=2)
            except Exception:
                pass

        last_step_ids = current_cycle_step_ids
        # æŒä¹…åŒ–æ‰§è¡Œé“¾åˆ°å­ä»»åŠ¡èŠ‚ç‚¹ï¼Œç¡®ä¿å­ä»»åŠ¡è¢«ä¸­æ–­åæ¢å¤æ—¶èƒ½ç»­æ¥æ‰§è¡Œé“¾
        graph_manager.update_subtask_last_step_ids(subtask_id, current_cycle_step_ids)

        if execution_tasks:
            tool_results = await asyncio.gather(*execution_tasks, return_exceptions=True)

            # Process results and check for immediate failures
            has_correctable_error = False
            correction_feedback = []
            observations = []
            MAX_OBSERVATION_LENGTH = EXECUTOR_MAX_OUTPUT_LENGTH
            truncated_steps = []

            for i, result in enumerate(tool_results):
                step_id = last_step_ids[i]
                tool_name = current_step_ops[i].get("action", {}).get("tool", "unknown_tool")
                step_status = "completed"
                
                # Handle errors
                if isinstance(result, Exception):
                    result_str = f"Error executing tool: {result}"
                    step_status = "failed"
                    if console_output_path:
                        try:
                            with open(console_output_path, "a", encoding="utf-8") as f:
                                f.write(f"[ERROR] å·¥å…· {tool_name} æ‰§è¡Œå¼‚å¸¸: {result}\n")
                        except Exception:
                            pass
                else:
                    result_str = str(result)
                    # Check for soft errors in JSON response
                    try:
                        data = json.loads(result_str)
                        if isinstance(data, dict) and data.get("success") is False:
                            error_type = data.get("error_type")
                            if error_type in ["SYNTAX", "MISSING_TOOL"]:
                                has_correctable_error = True
                                feedback = f"- Step {step_id} (Tool: {tool_name}) failed: {data.get('message')} -> {data.get('fix_suggestion')}"
                                correction_feedback.append(feedback)
                                step_status = "failed"
                    except:
                        pass

                # Truncation logic
                original_length = len(result_str)
                was_truncated = False
                if original_length > MAX_OBSERVATION_LENGTH:
                    result_str = result_str[:MAX_OBSERVATION_LENGTH] + f"\n... (Truncated from {original_length})"
                    was_truncated = True
                    _get_console().print(Panel(f"âš ï¸ åŠ¨ä½œ {step_id} ç»“æœè¿‡é•¿å·²æˆªæ–­", title="è­¦å‘Š", style="yellow"))
                    truncated_steps.append({
                        "step_id": step_id, 
                        "tool_name": tool_name, 
                        "original_length": original_length,
                        "sent_length": MAX_OBSERVATION_LENGTH
                    })

                observations.append(f"åŠ¨ä½œ {step_id} (å·¥å…·={tool_name}) çš„ç»“æœ: {result_str}")
                
                # Update graph and logs
                graph_manager.update_node(
                    step_id,
                    {
                        "observation": observations[-1],
                        "observation_truncated": was_truncated,
                        "observation_original_length": original_length,
                        "status": step_status,
                    },
                )
                
                try:
                    run_log_entry = {
                        "event": "executor_step_completed",
                        "step_id": step_id,
                        "tool_name": tool_name,
                        "result": result_str,
                        "timestamp": time.time(),
                    }
                    await broker.emit("execution.step.completed", run_log_entry, op_id=os.path.basename(log_dir) if log_dir else None)
                except:
                    pass

            # Handle immediate corrections
            if has_correctable_error:
                correction_prompt = f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨é”™è¯¯ï¼Œè¯·ç«‹å³ä¿®æ­£:\n" + "\n".join(correction_feedback)
                _get_console().print(Panel(correction_prompt, title="ğŸ¤– Executor: è¯·æ±‚ä¿®æ­£", style="bold yellow"))
                messages.append({"role": "user", "content": correction_prompt})
                continue

            full_observation = "\n".join(observations)
            messages.append({"role": "user", "content": f"ä½ å¹¶è¡Œæ‰§è¡Œäº† {len(last_step_ids)} ä¸ªåŠ¨ä½œï¼Œè§‚å¯Ÿåˆ°ï¼š\n{full_observation}"})

            if output_mode == "debug": # Changed from if verbose:
                _get_console().print(
                    Panel(
                        f"å·¥å…·æ‰§è¡Œç»“æœ:\n{full_observation}",
                        title="[bold green]Debug Tool Results[/bold green]", # Changed title
                        style="green"
                    )
                )

            if truncated_steps:
                messages.append({"role": "user", "content": f"âš ï¸ æ³¨æ„ï¼š{len(truncated_steps)} ä¸ªè§‚å¯Ÿç»“æœå·²è¢«æˆªæ–­ã€‚"})

        if is_final_step:
            _get_console().print(Panel(f"LLMå£°æ˜å­ä»»åŠ¡ {subtask_id} å·²å®Œæˆã€‚", style="green"))
            graph_manager.update_node(subtask_id, {"status": "completed"})
            return (subtask_id, "completed", cycle_metrics)

        # --- åŠ¨æ€ç»ˆæ­¢é€»è¾‘ ---
        # 1. æœ€å¤§æ­¥æ•°é™åˆ¶ï¼ˆå®‰å…¨ç½‘ï¼‰
        effective_max_steps = max_steps if max_steps is not None else EXECUTOR_MAX_STEPS
        if executed_steps_count >= effective_max_steps:
            _get_console().print(
                Panel(
                    f"è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ ({effective_max_steps})ï¼Œä¸ºå®‰å…¨èµ·è§ç»ˆæ­¢å­ä»»åŠ¡ã€‚",
                    title="æ™ºèƒ½ç»ˆæ­¢",
                    style="bold red",
                )
            )
            # å°†ç»ˆæ­¢åŸå› å†™å…¥å­ä»»åŠ¡èŠ‚ç‚¹ï¼Œä¾› Reflector/Planner ä½¿ç”¨
            graph_manager.update_node(
                subtask_id, {"termination_reason": "max_steps_reached", "executed_steps": executed_steps_count}
            )
            break

        # 2. æ£€æŸ¥æ–°äº§å‡ºç‰©
        if not llm_reply_json.get("staged_causal_nodes", []):
            consecutive_no_new_artifacts += 1
        else:
            consecutive_no_new_artifacts = 0  # æœ‰æ–°äº§å‡ºç‰©æ—¶é‡ç½®

        if not disable_artifact_check and consecutive_no_new_artifacts >= EXECUTOR_NO_ARTIFACTS_PATIENCE:
            _get_console().print(
                Panel(
                    f"è¿ç»­ {EXECUTOR_NO_ARTIFACTS_PATIENCE} æ­¥æ²¡æœ‰æ–°çš„äº§å‡ºç‰©æè®®ï¼Œæ¢ç´¢å·²åœæ»ã€‚ç»ˆæ­¢å­ä»»åŠ¡ã€‚",
                    title="æ™ºèƒ½ç»ˆæ­¢",
                    style="bold yellow",
                )
            )
            # è®°å½•ç¼ºä¹æ–°äº§ç‰©å¯¼è‡´çš„ç»ˆæ­¢åŸå› 
            graph_manager.update_node(
                subtask_id, {"termination_reason": "no_new_artifacts", "executed_steps": executed_steps_count}
            )
            break

        # 8. æ£€æŸ¥å¤–éƒ¨ç»ˆæ­¢ä¿¡å·
        halt_file = os.path.join(tempfile.gettempdir(), f"{graph_manager.task_id}.halt")
        if os.path.exists(halt_file):
            try:
                with open(halt_file, "r", encoding="utf-8") as f:
                    halt_payload = json.load(f) # Read payload to include in metrics if needed
                _get_console().print(
                    Panel(
                        f"ğŸš© åœ¨ {subtask_id} æ‰§è¡ŒæœŸé—´æ£€æµ‹åˆ°å¤–éƒ¨ç»ˆæ­¢ä¿¡å·ï¼æ­£åœ¨ä¸­æ–­...",
                        style="bold yellow",
                    )
                )
                for step_id in last_step_ids:
                    if graph_manager.graph.has_node(step_id):
                        graph_manager.update_node(step_id, {"status": "aborted"})
                # Save current messages state and turn counter before returning
                graph_manager.update_subtask_conversation_history(subtask_id, messages)
                return (subtask_id, "aborted_by_external_halt_signal", cycle_metrics)
            except Exception:
                _get_console().print(Panel("è¯»å–ç»ˆæ­¢ä¿¡å·æ–‡ä»¶å¤±è´¥æˆ–æ ¼å¼æ— æ•ˆï¼Œç»§ç»­æ‰§è¡Œã€‚", title="è­¦å‘Š", style="red"))

        # Save logs after each step
        if save_callback:
            save_callback(cycle_metrics=cycle_metrics)
        # æ–°å¢ï¼šå®æ—¶ç»´æŠ¤ metrics.json çš„ execution_steps å­—æ®µ
        if log_dir:
            metrics_path = os.path.join(log_dir, "metrics.json")
            try:
                if os.path.exists(metrics_path):
                    with open(metrics_path, "r", encoding="utf-8") as f:
                        metrics = json.load(f)
                else:
                    metrics = {}
                
                # ç›´æ¥è®¾ç½®æ‰§è¡Œæ­¥æ•°ï¼ˆä¸ç´¯åŠ ï¼‰
                metrics["execution_steps"] = executed_steps_count
                
                # å®æ—¶æ›´æ–°tool_callsï¼ˆä½¿ç”¨cycle_metricsä¸­çš„ç´¯è®¡å€¼ï¼‰
                if "tool_calls" not in metrics:
                    metrics["tool_calls"] = {}
                # å®æ—¶æ›´æ–° cost å’Œ token ä¿¡æ¯
                if "cost_cny" not in metrics:
                    metrics["cost_cny"] = 0
                metrics["cost_cny"] = max(metrics.get("cost_cny", 0), cycle_metrics.get("cost_cny", 0))
                metrics["total_tokens"] = cycle_metrics.get("prompt_tokens", 0) + cycle_metrics.get("completion_tokens", 0)
                
                # Atomic write
                tmp_path = metrics_path + ".tmp"
                with open(tmp_path, "w", encoding="utf-8") as f:
                    json.dump(metrics, f, ensure_ascii=False, indent=2)
                os.replace(tmp_path, metrics_path)
            except Exception:
                pass

        # Increment executed steps count for the next iteration
        executed_steps_count += 1


    # End of while loop
    _get_console().print(Panel(f"è¾¾åˆ°æœ€å¤§æ‰§è¡Œæ­¥æ•° {executed_steps_count}ï¼Œå­ä»»åŠ¡ç»“æŸã€‚", style="yellow"))
    for step_id in last_step_ids:
        if graph_manager.graph.has_node(step_id):
            graph_manager.update_node(step_id, {"status": "completed"})
    graph_manager.update_subtask_conversation_history(subtask_id, messages)
    cycle_metrics["execution_steps"] = executed_steps_count
    return (subtask_id, "completed", cycle_metrics)
