# core/reflector.py

from datetime import datetime
from typing import Any, List, Dict, Optional
import json
import re




def _get_console():
    """Lazy initialization of console to avoid circular imports."""
    from core.console import console_proxy
    return console_proxy
from llm.llm_client import LLMClient
from core.graph_manager import GraphManager
from rich.console import Console
from core.events import broker


class Reflector:
    """
    åæ€å™¨ï¼šè´Ÿè´£å¤ç›˜å·²å®Œæˆçš„å­ä»»åŠ¡ï¼Œå®¡æ ¸æ¥è‡ªæ‰§è¡Œå™¨çš„è§„åˆ’å»ºè®®ï¼Œ
    å¹¶ç”Ÿæˆæœ€ç»ˆçš„ã€ç»è¿‡éªŒè¯çš„å›¾æ“ä½œæŒ‡ä»¤ã€‚

    è¯¥ç±»å®ç°äº†P-E-Ræ¶æ„ä¸­çš„åæ€åŠŸèƒ½ï¼Œæ”¯æŒï¼š
    - å­ä»»åŠ¡å¤ç›˜ï¼šåˆ†ææ‰§è¡Œç»“æœï¼ŒéªŒè¯äº§å‡ºç‰©çš„æœ‰æ•ˆæ€§
    - å…¨å±€åæ€ï¼šå¯¹æ•´ä¸ªä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹è¿›è¡Œé«˜å±‚æ¬¡æ€»ç»“
    - æƒ…æŠ¥ç”Ÿæˆï¼šæå–æ”»å‡»æƒ…æŠ¥å’Œå¯æ“ä½œçš„æ´å¯Ÿ
    - ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šæ•´åˆå†å²åæ€è®°å½•å’ŒLLMæ¨ç†è¿‡ç¨‹

    Attributes:
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹ï¼Œç”¨äºç”Ÿæˆåæ€å†³ç­–
        console: Richæ§åˆ¶å°å®ä¾‹ï¼Œç”¨äºæ ¼å¼åŒ–è¾“å‡º
        _run_log_path: è¿è¡Œæ—¥å¿—æ–‡ä»¶è·¯å¾„
        _log_dir: æ—¥å¿—ç›®å½•è·¯å¾„
        _console_output_path: æ§åˆ¶å°è¾“å‡ºæ—¥å¿—è·¯å¾„
    """

    def __init__(self, llm_client: LLMClient, output_mode: str = "default"):
        self.llm_client = llm_client
        self.output_mode = output_mode # Store output_mode
        self.console = Console()  # åˆå§‹åŒ–æ§åˆ¶å°å®ä¾‹ç”¨äºæ ¼å¼åŒ–è¾“å‡º
        self._run_log_path = None
        self._log_dir = None

    def set_log_dir(self, log_dir: Optional[str]) -> None:
        """
        è®¾ç½®æ—¥å¿—ç›®å½•è·¯å¾„ã€‚

        Args:
            log_dir: æ—¥å¿—ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ç¦ç”¨æ—¥å¿—è®°å½•

        Returns:
            None
        """
        import os

        self._log_dir = log_dir
        self._run_log_path = os.path.join(log_dir, "run_log.json") if log_dir else None
        self._console_output_path = os.path.join(log_dir, "console_output.log") if log_dir else None

    def _generate_reflector_prompt(
        self,
        subtask_goal: str,
        status: str,
        execution_log: str,
        staged_causal_nodes: List[Dict],
        full_graph_summary: str,
        completion_criteria: str,
        dependency_context: Optional[List[Dict]] = None,
        failure_patterns_summary: Dict[str, Any] = None,
        *,
        reflector_context=None,
    ) -> str:
        """
        ä½¿ç”¨PromptManagerç”Ÿæˆåæ€å™¨æç¤ºè¯ï¼ˆå·²è¿ç§»åˆ°æ–°æ¨¡æ¿ç³»ç»Ÿï¼‰ã€‚

        Args:
            subtask_goal: å­ä»»åŠ¡ç›®æ ‡æè¿°
            status: å­ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
            execution_log: æ‰§è¡Œæ—¥å¿—
            staged_causal_nodes: æš‚å­˜çš„å› æœèŠ‚ç‚¹åˆ—è¡¨
            full_graph_summary: å®Œæ•´å›¾æ‘˜è¦
            completion_criteria: å®Œæˆæ ‡å‡†
            dependency_context: ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            failure_patterns_summary: å¤±è´¥æ¨¡å¼æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            reflector_context: åæ€ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼‰

        Returns:
            str: æ ¼å¼åŒ–åçš„åæ€å™¨æç¤ºè¯å­—ç¬¦ä¸²
        """
        from core.prompts import PromptManager

        manager = PromptManager()

        # æ„å»ºsubtaskå¯¹è±¡
        subtask = {"description": subtask_goal, "completion_criteria": completion_criteria}

        # æ„å»ºcontext
        context = {
            "causal_graph_summary": full_graph_summary or "å› æœé“¾å›¾è°±ä¸ºç©ºã€‚",
            "dependency_context": dependency_context or [],
            "failure_patterns": failure_patterns_summary,
        }

        # ä½¿ç”¨PromptManagerç”Ÿæˆæç¤ºè¯
        prompt = manager.build_reflector_prompt(
            subtask=subtask,
            status=status,
            execution_log=execution_log,
            staged_causal_nodes=staged_causal_nodes,
            context=context,
            reflector_context=reflector_context,
        )

        return prompt

    def _generate_reflection_context_section(self, reflector_context) -> str:
        """
        ç”Ÿæˆåæ€ä¸Šä¸‹æ–‡æ‘˜è¦éƒ¨åˆ†ã€‚

        æ•´åˆå·²éªŒè¯æ¨¡å¼ã€æŒä¹…æ€§æ´å¯Ÿã€ç›¸å…³åæ€å†å²å’ŒLLMåæ€è®°å½•ï¼Œ
        å½¢æˆå®Œæ•´çš„åæ€ä¸Šä¸‹æ–‡æ‘˜è¦ã€‚

        Args:
            reflector_context: åæ€ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ŒåŒ…å«å†å²åæ€å’ŒLLMæ¨ç†ä¿¡æ¯

        Returns:
            str: æ ¼å¼åŒ–çš„åæ€ä¸Šä¸‹æ–‡æ‘˜è¦å­—ç¬¦ä¸²
        """

        # ç”Ÿæˆå·²éªŒè¯æ¨¡å¼æ‘˜è¦
        validated_patterns_summary = self._generate_validated_patterns_summary(reflector_context)

        # ç”ŸæˆæŒä¹…æ€§æ´å¯Ÿæ‘˜è¦
        persistent_insights_summary = self._generate_persistent_insights_summary(reflector_context)

        # ç”Ÿæˆç›¸å…³åæ€å†å²
        relevant_reflection_log = self._generate_relevant_reflection_history(reflector_context)

        # ç”Ÿæˆå®Œæ•´LLMåæ€è®°å½•æ‘˜è¦
        llm_reflection_summary = self._generate_llm_reflection_summary(reflector_context)

        context_section = f"""
## å†å²åæ€ä¸Šä¸‹æ–‡ï¼ˆå¢å¼ºç‰ˆï¼‰

### å·²éªŒè¯çš„æœ‰æ•ˆæ¨¡å¼
{validated_patterns_summary}

### æŒä¹…æ€§æŠ€æœ¯æ´å¯Ÿ
{persistent_insights_summary}

### ç›¸å…³å†å²åæ€
{relevant_reflection_log}

### å®Œæ•´LLMåæ€è®°å½•
{llm_reflection_summary}
"""
        return context_section

    def _generate_validated_patterns_summary(self, reflector_context) -> str:
        """
        ç”Ÿæˆå·²éªŒè¯æ¨¡å¼æ‘˜è¦ã€‚

        Args:
            reflector_context: åæ€ä¸Šä¸‹æ–‡å¯¹è±¡

        Returns:
            str: å·²éªŒè¯çš„æœ‰æ•ˆæ¨¡å¼åˆ—è¡¨çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
        """
        if not reflector_context.validated_patterns:
            return "æš‚æ— å·²éªŒè¯çš„æœ‰æ•ˆæ¨¡å¼"

        summary = []
        for pattern in reflector_context.validated_patterns[-5:]:  # æœ€è¿‘5ä¸ªæ¨¡å¼
            summary.append(
                f"- {pattern.get('pattern_type', 'æœªçŸ¥æ¨¡å¼')}: {pattern.get('description', 'æ— æè¿°')} "
                f"(ç½®ä¿¡åº¦: {pattern.get('confidence', 0.0):.1f})"
            )
        return "\n".join(summary)

    def _generate_persistent_insights_summary(self, reflector_context) -> str:
        """
        ç”ŸæˆæŒä¹…æ€§æŠ€æœ¯æ´å¯Ÿæ‘˜è¦ã€‚

        Args:
            reflector_context: åæ€ä¸Šä¸‹æ–‡å¯¹è±¡

        Returns:
            str: æŒä¹…æ€§æŠ€æœ¯æ´å¯Ÿåˆ—è¡¨çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
        """
        if not reflector_context.persistent_insights:
            return "æš‚æ— æŒä¹…æ€§æŠ€æœ¯æ´å¯Ÿ"

        return "\n".join(
            [
                f"- {insight.get('insight_type', 'æœªçŸ¥æ´å¯Ÿ')}: {insight.get('description', 'æ— æè¿°')}"
                for insight in reflector_context.persistent_insights[-3:]
            ]
        )

    def _generate_relevant_reflection_history(self, reflector_context) -> str:
        """
        ç”Ÿæˆç›¸å…³å†å²åæ€è®°å½•ã€‚

        Args:
            reflector_context: åæ€ä¸Šä¸‹æ–‡å¯¹è±¡

        Returns:
            str: æœ€è¿‘3æ¬¡åæ€å°è¯•çš„æ‘˜è¦å­—ç¬¦ä¸²
        """
        if not reflector_context.reflection_log:
            return "æ— å†å²åæ€è®°å½•"

        summary = []
        for reflection in reflector_context.reflection_log:
            ts = getattr(reflection, "timestamp", 0) or 0
            sub_id = getattr(reflection, "subtask_id", "æœªçŸ¥ä»»åŠ¡")
            key_insight = getattr(reflection, "key_insight", "")
            rep = getattr(reflection, "full_reflection_report", None)
            status = None
            finding = None
            action = None
            artifacts_count = None
            if isinstance(rep, dict):
                audit = rep.get("audit_result", {})
                status = audit.get("status")
                kfs = rep.get("key_findings")
                if isinstance(kfs, list) and kfs:
                    finding = kfs[0] if isinstance(kfs[0], str) else str(kfs[0])
                intel = rep.get("attack_intelligence", {})
                acts = intel.get("actionable_insights")
                if isinstance(acts, list) and acts:
                    action = acts[0]
                # éªŒè¯èŠ‚ç‚¹ä¿¡æ¯ï¼ˆæ›¿ä»£æ—§çš„validated_artifactsï¼‰
                nodes = rep.get("validated_nodes")
                if isinstance(nodes, list):
                    artifacts_count = len(nodes)
            timestamp = datetime.fromtimestamp(float(ts)).strftime("%H:%M:%S")
            parts = [f"- {timestamp}: {sub_id}"]
            if status:
                parts.append(f"çŠ¶æ€: {status}")
            if key_insight:
                parts.append(f"æ´å¯Ÿ: {key_insight}")
            if finding:
                parts.append(f"å‘ç°: {finding}")
            if action:
                parts.append(f"å»ºè®®: {action}")
            if artifacts_count is not None:
                parts.append(f"äº§å‡ºç‰©: {artifacts_count}")
            summary.append(" | ".join(parts))
        return "\n".join(summary)

    def _extract_audit_summary(self, audit_result: dict, summary: list) -> None:
        """
        ä»å®¡è®¡ç»“æœä¸­æå–å…³é”®ä¿¡æ¯åˆ°æ‘˜è¦ã€‚

        Args:
            audit_result: å®¡è®¡ç»“æœå­—å…¸
            summary: æ‘˜è¦åˆ—è¡¨ï¼Œç”¨äºé™„åŠ æå–çš„ä¿¡æ¯
        """
        if not audit_result:
            return

        status = audit_result.get("status", "unknown")
        completion = audit_result.get("completion_check", "")
        strategic_failure = audit_result.get("is_strategic_failure", False)

        summary.append(f"å®¡è®¡çŠ¶æ€: {status}")
        if completion:
            completion_preview = completion[:100] + "..." if len(completion) > 100 else completion
            summary.append(f"å®Œæˆåº¦æ£€æŸ¥: {completion_preview}")
        if strategic_failure:
            summary.append("æˆ˜ç•¥æ€§å¤±è´¥: æ˜¯")

        methodology_issues = audit_result.get("methodology_issues", [])
        if methodology_issues:
            issues_summary = ", ".join(
                [f"{issue[:30]}..." if len(issue) > 30 else issue for issue in methodology_issues[:2]]
            )
            if len(methodology_issues) > 2:
                issues_summary += f" ç­‰{len(methodology_issues)}ä¸ªæ–¹æ³•è®ºé—®é¢˜"
            summary.append(f"æ–¹æ³•è®ºé—®é¢˜: {issues_summary}")

        logic_issues = audit_result.get("logic_issues", [])
        if logic_issues:
            logic_summary = ", ".join([f"{issue[:30]}..." if len(issue) > 30 else issue for issue in logic_issues[:2]])
            if len(logic_issues) > 2:
                logic_summary += f" ç­‰{len(logic_issues)}ä¸ªé€»è¾‘é—®é¢˜"
            summary.append(f"é€»è¾‘é—®é¢˜: {logic_summary}")

    def _extract_attack_intelligence(self, attack_intelligence: dict, summary: list) -> None:
        """
        ä»æ”¿å‡»æƒ…æŠ¥ä¸­æå–å¯æ‰§è¡Œæ´å¯Ÿã€‚

        Args:
            attack_intelligence: æ”»å‡»æƒ…æŠ¥å­—å…¸
            summary: æ‘˜è¦åˆ—è¡¨ï¼Œç”¨äºé™„åŠ æå–çš„ä¿¡æ¯
        """
        if not attack_intelligence:
            return

        actionable_insights = attack_intelligence.get("actionable_insights", [])
        if actionable_insights:
            insights_summary = ", ".join(
                [f"{insight[:40]}..." if len(insight) > 40 else insight for insight in actionable_insights[:2]]
            )
            if len(actionable_insights) > 2:
                insights_summary += f" ç­‰{len(actionable_insights)}ä¸ªå¯æ‰§è¡Œæ´å¯Ÿ"
            summary.append(f"æ”»å‡»æƒ…æŠ¥: {insights_summary}")

    def _extract_key_facts(self, key_facts: list, summary: list) -> None:
        """
        ä»å…³é”®äº‹å®åˆ—è¡¨ä¸­æå–æ‘˜è¦ã€‚

        Args:
            key_facts: å…³é”®äº‹å®åˆ—è¡¨
            summary: æ‘˜è¦åˆ—è¡¨ï¼Œç”¨äºé™„åŠ æå–çš„ä¿¡æ¯
        """
        if not key_facts:
            return

        facts_summary = ", ".join([f"{fact[:50]}..." if len(fact) > 50 else fact for fact in key_facts[:3]])
        if len(key_facts) > 3:
            facts_summary += f" ç­‰{len(key_facts)}ä¸ªå…³é”®äº‹å®"
        summary.append(f"å…³é”®äº‹å®: {facts_summary}")

    def _extract_causal_updates(self, causal_updates: dict, summary: list) -> None:
        """
        æå–å› æœå›¾æ›´æ–°ç±»å‹ã€‚

        Args:
            causal_updates: å› æœå›¾æ›´æ–°å­—å…¸
            summary: æ‘˜è¦åˆ—è¡¨ï¼Œç”¨äºé™„åŠ æå–çš„ä¿¡æ¯
        """
        if not causal_updates:
            return

        update_types = list(causal_updates.keys())
        summary.append(f"å› æœå›¾æ›´æ–°ç±»å‹: {', '.join(update_types)}")

    def _extract_prompt_context(self, llm_reflection_prompt: str, summary: list) -> None:
        """
        ä»LLMåæ€æç¤ºè¯ä¸­æå–è§’è‰²å’Œæ ¸å¿ƒèŒè´£ã€‚

        Args:
            llm_reflection_prompt: LLMåæ€æç¤ºè¯å­—ç¬¦ä¸²
            summary: æ‘˜è¦åˆ—è¡¨ï¼Œç”¨äºé™„åŠ æå–çš„ä¿¡æ¯
        """
        if not llm_reflection_prompt:
            return

        role_match = re.search(r"# è§’è‰²: ([^\n]+)", llm_reflection_prompt)
        if role_match:
            summary.append(f"åæ€è§’è‰²: {role_match.group(1)}")

        duties_match = re.search(r"## æ ¸å¿ƒèŒè´£\\s+([^#]+)", llm_reflection_prompt, re.DOTALL)
        if duties_match:
            duties = duties_match.group(1).strip()
            duties_preview = duties[:100] + "..." if len(duties) > 100 else duties
            summary.append(f"æ ¸å¿ƒèŒè´£: {duties_preview}")

    def _extract_response_content(self, resp_text: str, summary: list) -> None:
        """
        ä»LLMåæ€å“åº”ä¸­æå–ç»“æ„åŒ–å†…å®¹ã€‚

        Args:
            resp_text: LLMåæ€å“åº”æ–‡æœ¬
            summary: æ‘˜è¦åˆ—è¡¨ï¼Œç”¨äºé™„åŠ æå–çš„ä¿¡æ¯
        """
        try:
            resp_data = json.loads(resp_text)
            if isinstance(resp_data, dict):
                audit_result = resp_data.get("audit_result", {})
                if audit_result:
                    status = audit_result.get("status", "unknown")
                    summary.append(f"å“åº”çŠ¶æ€: {status}")

                    recommendations = resp_data.get("recommendations", [])
                    if recommendations:
                        rec_summary = ", ".join(
                            [rec[:50] + "..." if len(rec) > 50 else rec for rec in recommendations[:2]]
                        )
                        summary.append(f"å…³é”®å»ºè®®: {rec_summary}")
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯JSONï¼Œæå–æ–‡æœ¬ä¸­çš„å…³é”®ä¿¡æ¯
            lines = resp_text.split("\n")
            key_lines = [
                line
                for line in lines
                if any(
                    keyword in line
                    for keyword in [
                        "æ¼æ´",
                        "æ¼æ´",
                        "vulnerability",
                        "Vulnerability",
                        "å»ºè®®",
                        "recommendation",
                        "Recommendation",
                    ]
                )
            ]
            if key_lines:
                key_info = "; ".join([line[:80] + "..." if len(line) > 80 else line for line in key_lines[:3]])
                summary.append(f"å“åº”å…³é”®ä¿¡æ¯: {key_info}")

    def _generate_llm_reflection_summary(self, reflector_context) -> str:
        """
        ç”Ÿæˆå®Œæ•´LLMåæ€è®°å½•æ‘˜è¦ã€‚

        Args:
            reflector_context: åæ€ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ŒåŒ…å«LLMæ¨ç†å†å²

        Returns:
            LLMè¾“å…¥æç¤ºè¯ã€è¾“å‡ºå“åº”å’Œæ¨ç†è¿‡ç¨‹çš„æ ¼å¼åŒ–æ‘˜è¦
        """
        if not reflector_context.reflection_log:
            return "æš‚æ— LLMåæ€è®°å½•"

        # è·å–æœ€è¿‘çš„åæ€è®°å½•
        latest_reflection = reflector_context.reflection_log[-1]
        summary = []

        # æå–å…³é”®ä¿¡æ¯ï¼šä»å®Œæ•´åæ€æŠ¥å‘Šä¸­æå–æ ¸å¿ƒæ´å¯Ÿ
        if hasattr(latest_reflection, "full_reflection_report") and latest_reflection.full_reflection_report:
            rep = latest_reflection.full_reflection_report
            if isinstance(rep, dict):
                self._extract_audit_summary(rep.get("audit_result", {}), summary)
                self._extract_attack_intelligence(rep.get("attack_intelligence", {}), summary)
                self._extract_key_facts(rep.get("key_facts", []), summary)
                self._extract_causal_updates(rep.get("causal_graph_updates", {}), summary)

        # ä¼˜åŒ–LLMåæ€æç¤ºè¯æ‘˜è¦ - æå–è§’è‰²å’Œæ ¸å¿ƒèŒè´£
        if hasattr(latest_reflection, "llm_reflection_prompt") and latest_reflection.llm_reflection_prompt:
            self._extract_prompt_context(latest_reflection.llm_reflection_prompt, summary)

        # ä¼˜åŒ–LLMåæ€å“åº”æ‘˜è¦ - æå–ç»“æ„åŒ–å“åº”å†…å®¹
        if hasattr(latest_reflection, "llm_reflection_response") and latest_reflection.llm_reflection_response:
            self._extract_response_content(latest_reflection.llm_reflection_response, summary)

        return " | ".join(summary) if summary else "LLMåæ€è®°å½•è¯¦æƒ…å¾…å®Œå–„"

    def _normalize_dependency_context(self, dependency_context: Optional[List[Dict]], subtask_data: Dict) -> Optional[List[Dict]]:
        termination_reason = subtask_data.get("termination_reason")
        executed_steps = subtask_data.get("executed_steps")
        if dependency_context and isinstance(dependency_context, list):
            dependency_context = [
                item for item in dependency_context if not (isinstance(item, dict) and "termination_reason" in item)
            ]
        if termination_reason:
            dependency_context = dependency_context or []
            dependency_context.append(
                {
                    "termination_reason": termination_reason,
                    "executed_steps": executed_steps,
                    "source": "normalized",
                }
            )
        return dependency_context

    async def _evaluate_success_with_llm(self, completion_criteria: str, execution_log: str) -> bool:
        """
        ä½¿ç”¨LLMæ¥éªŒè¯ä»»åŠ¡æ˜¯å¦æˆåŠŸï¼Œå–ä»£å›ºå®šçš„æ­£åˆ™è¡¨è¾¾å¼ã€‚

        Args:
            completion_criteria: ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€å®Œæˆæ ‡å‡†ã€‚
            execution_log: æ‰§è¡Œå™¨çš„è¾“å‡ºæ—¥å¿—ã€‚

        Returns:
            bool: å¦‚æœLLMåˆ¤æ–­ä»»åŠ¡å·²æˆåŠŸï¼Œåˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseã€‚
        """
        if not completion_criteria or not execution_log:
            return False

        try:
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„æ¸—é€æµ‹è¯•ç»“æœå®¡è®¡å‘˜ã€‚
- ä»»åŠ¡çš„å®Œæˆæ ‡å‡†æ˜¯ï¼š"{completion_criteria}"
- æ‰§è¡Œæ—¥å¿—å’Œç»“æœå¦‚ä¸‹ï¼š"{execution_log}"

åŸºäºä¸Šè¿°æ—¥å¿—ï¼Œè¯·åˆ¤æ–­å®Œæˆæ ‡å‡†æ˜¯å¦å·²æ˜ç¡®ä¸”æ— æ­§ä¹‰åœ°è¾¾æˆï¼Ÿ
ä»…å›ç­” "true" æˆ– "false"ã€‚
"""
            messages = [{"role": "user", "content": prompt}]
            
            # ä½¿ç”¨ä¸€ä¸ªä¸“ç”¨çš„ã€è½»é‡çº§çš„éªŒè¯è§’è‰²
            response, _ = await self.llm_client.send_message(messages, role="reflector_validator")
            
            # è§£æLLMçš„å¸ƒå°”å€¼å“åº”
            result_str = str(response).strip().lower()
            _get_console().print(f"ğŸ•µï¸  åŸºäºLLMçš„éªŒè¯è¿”å›: [cyan]'{result_str}'[/cyan]", style="dim")
            return result_str == "true"
            
        except Exception as e:
            _get_console().print(f"âš ï¸ åŸºäºLLMçš„éªŒè¯å¤±è´¥: {e}", style="yellow")
            return False


    async def reflect(
        self,
        subtask_id: str,  # Add subtask_id as a parameter
        subtask_data: Dict,
        status: str,
        execution_log: str,
        proposed_changes: List[Dict],
        staged_causal_nodes: List[Dict],
        full_graph_summary: str,
        dependency_context: Optional[List[Dict]] = None,
        graph_manager=None,  # Add graph_manager to access causal graph analysis
        reflector_context=None,  # æ–°å¢ï¼šReflectorä¸Šä¸‹æ–‡å¯¹è±¡
    ) -> Dict:
        """
        æ‰§è¡Œåæ€ä¸å®¡æ ¸ã€‚

        è¯¥å‡½æ•°å®ç°äº†åæ€å™¨çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
        - åˆ†æå­ä»»åŠ¡æ‰§è¡Œç»“æœå’ŒçŠ¶æ€
        - éªŒè¯äº§å‡ºç‰©çš„æœ‰æ•ˆæ€§å’Œå®Œæ•´æ€§
        - ç”Ÿæˆæ”»å‡»æƒ…æŠ¥å’Œå¯æ“ä½œçš„æ´å¯Ÿ
        - æä¾›å› æœå›¾æ›´æ–°å»ºè®®
        - æ”¯æŒå¤±è´¥æ¨¡å¼åˆ†æå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥

        Args:
            subtask_id: å­ä»»åŠ¡ID
            subtask_data: å­ä»»åŠ¡æ•°æ®å­—å…¸
            status: å­ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
            execution_log: æ‰§è¡Œæ—¥å¿—
            proposed_changes: æè®®çš„å˜æ›´åˆ—è¡¨
            staged_causal_nodes: æš‚å­˜çš„å› æœèŠ‚ç‚¹åˆ—è¡¨
            full_graph_summary: å®Œæ•´å›¾æ‘˜è¦
            long_mem: é•¿æœŸè®°å¿†å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            dependency_context: ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            graph_manager: å›¾ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            reflector_context: åæ€ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼‰

        Returns:
            åæ€ç»“æœå­—å…¸ï¼ŒåŒ…å«å®¡æ ¸ç»“æœã€æƒ…æŠ¥æ‘˜è¦ã€æŒ‡æ ‡ç­‰
        """
        subtask_goal = subtask_data.get("id", subtask_id)
        completion_criteria = subtask_data.get("completion_criteria", "No specific criteria defined.")

        failure_patterns_summary = {}
        if graph_manager:
            failure_patterns_summary = graph_manager.analyze_failure_patterns()

        dependency_context = self._normalize_dependency_context(dependency_context, subtask_data)
        prompt = self._generate_reflector_prompt(
            subtask_goal,
            status,
            execution_log,
            staged_causal_nodes,
            full_graph_summary,
            completion_criteria,
            dependency_context,
            failure_patterns_summary,
            reflector_context=reflector_context,
        )
        messages = [{"role": "user", "content": prompt}]

        try:
            reflection_data, call_metrics = await self.llm_client.send_message(messages, role="reflector")
            if not reflection_data:
                raise ValueError("LLM returned no data for reflection.")

            reflection_data["metrics"] = call_metrics
            reflection_data["llm_reflection_prompt"] = prompt

            audit_result = reflection_data.get("audit_result", {})

            # ç›´æ¥é‡‡ç”¨LLMçš„åˆ¤æ–­ç»“æœï¼Œç”±Plannerå†³å®šä»»åŠ¡æ˜¯å¦å·²å®Œæˆ
            llm_reported_status = audit_result.get("status", "").upper()
            _get_console().print(f"ğŸ¤– LLM reported status: [bold green]{llm_reported_status}[/bold green]. Directly adopting LLM judgment.", style="dim")

            # ä¿æŒå¯¹ validated_nodes çš„å¼•ç”¨ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½åŒ…å«é™¤ç›®æ ‡äº§ç‰©ä¹‹å¤–çš„å…¶ä»–æœ‰ç”¨è¯æ®
            reflection_data.setdefault("causal_graph_updates", {})
            try:
                import os

                op_id = os.path.basename(self._log_dir) if self._log_dir else None
                await broker.emit("reflection.completed", {"subtask_id": subtask_id}, op_id=op_id)
            except Exception:
                pass
            return reflection_data

        except (json.JSONDecodeError, ValueError) as e:
            # è®°å½•å¼‚å¸¸åˆ° console_output.log
            if hasattr(self, "_console_output_path") and self._console_output_path:
                try:
                    with open(self._console_output_path, "a", encoding="utf-8") as f:
                        f.write(f"[ERROR] Reflectorå¼‚å¸¸: {type(e).__name__}: {e}\n")
                except Exception:
                    pass
            print(f"è§£æReflectorè¾“å‡ºå¤±è´¥: {e}")
            try:
                import os

                op_id = os.path.basename(self._log_dir) if self._log_dir else None
                await broker.emit("reflection.completed", {"subtask_id": subtask_id, "error": str(e)}, op_id=op_id)
            except Exception:
                pass
            return {
                "audit_result": {
                    "status": "FAILED",
                    "completion_check": "è§£æå¤±è´¥",
                    "logic_issues": [str(e)],
                    "methodology_issues": [],
                },
                "key_findings": [],
                "validated_nodes": [],
                "insight": None,
                "causal_graph_updates": {},
                "metrics": None,
            }

    def _generate_global_reflector_prompt(self, simplified_graph: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆç”¨äºå…¨å±€åæ€çš„æç¤ºè¯ï¼Œä»¥æç‚¼å¯å¤ç”¨çš„STEç»éªŒã€‚

        è¯¥æ–¹æ³•åˆ†æç®€åŒ–çš„å› æœå›¾ï¼Œç”Ÿæˆé’ˆå¯¹å…¨å±€åæ€çš„æç¤ºè¯ï¼Œç”¨äºï¼š
        - è¯†åˆ«æˆåŠŸçš„æ”»å‡»æ¨¡å¼å’Œç­–ç•¥
        - æå–å¯å¤ç”¨çš„æˆ˜æœ¯çŸ¥è¯†
        - åˆ†æå¤±è´¥åŸå› å’Œæ”¹è¿›å»ºè®®
        - ç”Ÿæˆç­–ç•¥-æˆ˜æœ¯-ç¤ºä¾‹(STE)çŸ¥è¯†æ¡†æ¶

        Args:
            simplified_graph: ç®€åŒ–çš„å› æœå›¾å­—å…¸ï¼ŒåŒ…å«èŠ‚ç‚¹å’Œè¾¹ä¿¡æ¯

        Returns:
            å…¨å±€åæ€æç¤ºè¯å­—ç¬¦ä¸²ï¼ŒåŒ…å«STEç»éªŒæå–æŒ‡å¯¼
        """
        simplified_graph_json = json.dumps(simplified_graph, indent=2, ensure_ascii=False)

        return f"""# è§’è‰²ï¼šé¦–å¸­æ¸—é€æµ‹è¯•æˆ˜ç•¥å®¶ä¸çŸ¥è¯†å·¥ç¨‹å¸ˆ

## æ ¸å¿ƒç›®æ ‡ï¼š
ä»ä¸€ä¸ªå·²å®Œæˆçš„ã€æˆåŠŸçš„æ”»å‡»ä»»åŠ¡ä¸­ï¼Œæç‚¼å‡ºä¸€ä¸ªç»“æ„åŒ–çš„ã€å¯å¤ç”¨çš„ç»éªŒï¼Œæ ¼å¼ä¸ºâ€œæˆ˜ç•¥-æˆ˜æœ¯-æ¡ˆä¾‹â€(STE)ã€‚

## å¾…åˆ†æçš„æˆåŠŸæ”»å‡»æ¡ˆä¾‹ (ç®€åŒ–è½¨è¿¹)ï¼š

```json
{simplified_graph_json}
```

## æç‚¼æŒ‡ä»¤ï¼š

ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼Œå°†ä¸Šè¿°æˆåŠŸæ¡ˆä¾‹æç‚¼æˆä¸€ä¸ªSTEç»éªŒå¯¹è±¡ï¼š

### 1. è¯„ä¼°æˆåŠŸè´¨é‡ (CRITICAL)
ä½ å¿…é¡»é¦–å…ˆæ£€æŸ¥ `simplified_graph_json` ä¸­æ˜¯å¦å­˜åœ¨ `node_type` ä¸º `ConfirmedVulnerability` çš„èŠ‚ç‚¹ã€‚
- **å¦‚æœå­˜åœ¨ `ConfirmedVulnerability`**ï¼šè¿™ä»£è¡¨äº†ä¸€æ¬¡é«˜è´¨é‡çš„ã€å·²éªŒè¯çš„æˆåŠŸæ”»å‡»ã€‚ä½ çš„åˆ†æ**å¿…é¡»**å›´ç»•å¯¼è‡´è¿™ä¸ªèŠ‚ç‚¹çš„æ”»å‡»è·¯å¾„å±•å¼€ã€‚åœ¨ `global_summary` ä¸­æ˜ç¡®æŒ‡å‡ºè¿™æ˜¯ä¸€ä¸ªå·²ç¡®è®¤çš„æ¼æ´ã€‚
- **å¦‚æœä¸å­˜åœ¨ `ConfirmedVulnerability`**ï¼šè¿™å¯èƒ½æ˜¯ä¸€æ¬¡å¶ç„¶çš„æˆåŠŸï¼ˆä¾‹å¦‚ï¼Œä»…æ‰¾åˆ°flagä½†æœªç†è§£æ¼æ´ï¼‰ï¼Œæˆ–è€…æ˜¯ä¸€æ¬¡å¸¸è§„çš„ä¿¡æ¯æ”¶é›†ã€‚ä½ çš„åˆ†æåº”ä¾§é‡äºå…¶æœºä¼šä¸»ä¹‰æ€§è´¨å’Œæ½œåœ¨çš„æ”¹è¿›ç©ºé—´ã€‚

### 2. æç‚¼æˆ˜ç•¥åŸåˆ™ (Strategic Principle)
- è¿™æ˜¯æœ€é«˜å±‚æ¬¡çš„ã€ä¸€å¥è¯çš„æ”»å‡»åŸåˆ™ã€‚
- å®ƒåº”è¯¥å›ç­”â€œä¸ºä»€ä¹ˆï¼ˆWhyï¼‰â€å¯ä»¥è¿™ä¹ˆåšï¼Œæ­ç¤ºäº†å“ªä¸€ç±»æ ¹æœ¬æ€§çš„å®‰å…¨å¼±ç‚¹ã€‚
- **ç¤ºä¾‹**ï¼šâ€œå½“è®¤è¯ä»¤ç‰Œä½¿ç”¨æ— MACçš„CBCæ¨¡å¼åŠ å¯†æ—¶ï¼Œå¯é€šè¿‡ç¯¡æ”¹IVæˆ–å‰ç½®å¯†æ–‡å—æ¥ä¼ªé€ èº«ä»½ã€‚â€

### 3. æç‚¼æˆ˜æœ¯æ‰‹å†Œ (Tactical Playbook)
- è¿™æ˜¯å®ç°è¯¥æˆ˜ç•¥çš„ã€æœ‰åºçš„ã€æŠ½è±¡çš„æ­¥éª¤åˆ—è¡¨ã€‚
- å®ƒåº”è¯¥å›ç­”â€œå¦‚ä½•åšï¼ˆHowï¼‰â€çš„æ­¥éª¤ã€‚
- æ¯ä¸ªæ­¥éª¤éƒ½åº”è¯¥æ˜¯ä¸€ä¸ªåŠ¨è¯çŸ­è¯­ï¼Œæè¿°ä¸€ä¸ªæˆ˜æœ¯ç›®æ ‡ï¼Œè€Œä¸æ˜¯å…·ä½“çš„å·¥å…·è°ƒç”¨ã€‚
- **ç¤ºä¾‹**ï¼š
  ```json
  [
    "ä¿¡æ¯æ”¶é›†ï¼šè·å–åŸå§‹åŠ å¯†ä»¤ç‰Œ",
    "ç»“æ„åˆ†æï¼šè¯†åˆ«åŠ å¯†æ¨¡å¼ã€å—å¤§å°å’Œæ˜æ–‡æ ¼å¼",
    "è½½è·æ„é€ ï¼šè®¡ç®—å¹¶ç”Ÿæˆç¯¡æ”¹åçš„åŠ å¯†ä»¤ç‰Œ",
    "æ”»å‡»æ‰§è¡Œï¼šä½¿ç”¨ç¯¡æ”¹åçš„ä»¤ç‰Œè®¿é—®å—ä¿æŠ¤èµ„æº"
  ]
  ```

### 4. å®šä¹‰é€‚ç”¨åœºæ™¯ (Applicability)
- è¿™æ˜¯ä¸€ä¸ªæ ‡ç­¾åˆ—è¡¨ï¼Œå®šä¹‰äº†è¯¥STEç»éªŒæœ€å¯èƒ½åœ¨å“ªäº›åœºæ™¯ä¸‹è¢«å¤ç”¨ã€‚
- **ç¤ºä¾‹**ï¼š`["web_security", "session_hijacking", "cbc_bit_flipping", "ctf"]`

## è¾“å‡ºæ ¼å¼ (ä»…é™JSON):

ä½ **å¿…é¡»**è¾“å‡ºä¸€ä¸ªç»“æ„åˆæ³•çš„ JSON å¯¹è±¡ï¼Œå…¶ä¸­å¿…é¡»åŒ…å« `global_summary`, `strategic_analysis`, å’Œ `global_insight` é”®ã€‚`global_insight` å¿…é¡»ä¸¥æ ¼éµå¾ªSTEæ ¼å¼ã€‚

{{
  "global_summary": "ç”¨ä¸€å¥è¯æ€»ç»“æ•´ä¸ªä»»åŠ¡çš„æ ¸å¿ƒæˆ˜å½¹è·¯å¾„å’Œæœ€ç»ˆç»“æœã€‚",
  "strategic_analysis": "å¯¹æ•´ä½“æˆ˜ç•¥çš„è¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬è§„åˆ’ã€æ‰§è¡Œå’Œåæ€çš„äº®ç‚¹ä¸ä¸è¶³ã€‚",
  "global_insight": {{
    "strategic_principle": "æ­¤å¤„å¡«å†™ä½ æç‚¼çš„æˆ˜ç•¥åŸåˆ™ã€‚",
    "tactical_playbook": [
      "æ­¤å¤„å¡«å†™ç¬¬ä¸€ä¸ªæˆ˜æœ¯æ­¥éª¤",
      "æ­¤å¤„å¡«å†™ç¬¬äºŒä¸ªæˆ˜æœ¯æ­¥éª¤",
      "..."
    ],
    "applicability": ["tag1", "tag2", "..."]
  }}
}}
"""

    async def reflect_global(self, graph_manager: GraphManager) -> Dict:
        """
        æ‰§è¡Œå…¨å±€åæ€ï¼Œç”Ÿæˆæœ€é«˜å±‚æ¬¡çš„æˆ˜ç•¥æ´è§å’Œç»éªŒæ€»ç»“ã€‚

        è¯¥å‡½æ•°å®ç°äº†å¯¹æ•´ä¸ªä»»åŠ¡å›¾è°±çš„å…¨å±€åæ€åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
        - æ£€æŸ¥ä»»åŠ¡ç›®æ ‡æ˜¯å¦è¾¾æˆ
        - ç®€åŒ–å› æœå›¾å¹¶ç”Ÿæˆå…¨å±€åæ€æç¤ºè¯
        - è°ƒç”¨LLMç”Ÿæˆæˆ˜ç•¥åˆ†æå’Œå…¨å±€æ´å¯Ÿ
        - æå–å¯å¤ç”¨çš„STEï¼ˆç­–ç•¥-æˆ˜æœ¯-ç¤ºä¾‹ï¼‰ç»éªŒ

        Args:
            graph_manager: å›¾ç®¡ç†å™¨å®ä¾‹ï¼Œæä¾›ä»»åŠ¡å›¾è°±å’ŒçŠ¶æ€ä¿¡æ¯

        Returns:
            å…¨å±€åæ€ç»“æœå­—å…¸ï¼ŒåŒ…å«æˆ˜ç•¥åˆ†æã€å…¨å±€æ´å¯Ÿã€æŒ‡æ ‡ç­‰
        """
        if not graph_manager.is_goal_achieved():
            return {
                "global_summary": "ä»»åŠ¡æœªæˆåŠŸï¼Œè·³è¿‡å…¨å±€ç»éªŒå½’æ¡£ã€‚",
                "strategic_analysis": "",
                "global_insight": None,
                "metrics": None,
            }

        simplified_graph = graph_manager.get_simplified_graph()
        prompt = self._generate_global_reflector_prompt(simplified_graph)
        messages = [{"role": "user", "content": prompt}]

        try:
            response, call_metrics = await self.llm_client.send_message(messages, role="reflector")
            if not response:
                raise ValueError("LLM returned no data for global reflection.")

            # response is already a dictionary from llm_client, not a JSON string
            global_reflection_data = response
            global_reflection_data["metrics"] = call_metrics

            if global_reflection_data.get("global_insight"):
                global_reflection_data["global_insight"]["example_trajectory"] = simplified_graph

            return global_reflection_data

        except (json.JSONDecodeError, ValueError) as e:
            print(f"è§£æGlobal Reflectorè¾“å‡ºå¤±è´¥: {e}")
            return {
                "global_summary": "å…¨å±€åæ€å¤±è´¥ï¼Œæ— æ³•è§£æLLMè¾“å‡ºã€‚",
                "strategic_analysis": "",
                "global_insight": None,
                "metrics": None,
            }
