# llm_client.py
# LLMæ¨ç†APIå°è£…ï¼Œæ”¯æŒQwenç³»åˆ—OpenAIå…¼å®¹API
# ä¾èµ– openai>=1.0.0

import sys
import asyncio

from typing import Any, Dict, List, Optional
import httpx
import json
import re
from rich.panel import Panel
from conf.config import (
    LLM_PROVIDER,
    LLM_API_BASE_URL,
    LLM_API_KEY,
    LLM_FALLBACK_API_KEY,
    LLM_MODELS,
    LLM_TEMPERATURES,
    LLM_EXTRA_BODY_ENABLED,
    LLM_THINKING,
    ANTHROPIC_API_BASE_URL,
    ANTHROPIC_API_KEY,
    ANTHROPIC_FALLBACK_API_KEY,
    ANTHROPIC_MODELS,
    ANTHROPIC_VERSION,
)

# å¯¼å…¥äº‹ä»¶ä»£ç†
try:
    from core.events import broker
except ImportError:
    broker = None


class LLMClient:
    def __init__(self, op_id: Optional[str] = None):
        self.provider = LLM_PROVIDER
        self.op_id = op_id  # ç”¨äºäº‹ä»¶å‘é€

        # Token prices in CNY per 1000 tokens
        # ä½¿ç”¨é€šç”¨é¢„ä¼°ä»·æ ¼ï¼Œé¿å…ç¡¬ç¼–ç ç‰¹å®šæ¨¡å‹ã€‚ç”¨æˆ·åº”æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´ã€‚
        self.model_token_prices_cny = {
            "default": {"input": 0.004, "output": 0.016},  # Default placeholder prices
        }
        # self.cny_to_usd_rate = 7.0 # No longer needed as all costs will be in CNY.
        if self.provider == "anthropic":
            self.api_url = ANTHROPIC_API_BASE_URL
            self.api_key = ANTHROPIC_API_KEY
            self.fallback_api_key = ANTHROPIC_FALLBACK_API_KEY
            self.models = ANTHROPIC_MODELS
            self.anthropic_version = ANTHROPIC_VERSION
            # NOTE: Using placeholder costs for Anthropic models. Replace with actuals.
            self.prompt_token_cost = 0.000003  # $3 per million tokens
            self.completion_token_cost = 0.000015  # $15 per million tokens
        else:  # Default to openai
            self.api_url = LLM_API_BASE_URL
            self.api_key = LLM_API_KEY
            self.fallback_api_key = LLM_FALLBACK_API_KEY
            self.models = LLM_MODELS
            # NOTE: Using placeholder costs for OpenAI-compatible models. Replace with actuals.
            self.prompt_token_cost = 0.000001  # $1 per million tokens
            self.completion_token_cost = 0.000002  # $2 per million tokens

        self.temperatures = LLM_TEMPERATURES
        self.client = httpx.AsyncClient()
        self.console = None  # Will initialize console when needed
        self.reset_metrics()

    def _get_console(self):
        """Lazy initialization of console to avoid circular imports."""
        if self.console is None:
            from core.console import console_proxy
            self.console = console_proxy
        return self.console

    def reset_metrics(self):
        """Resets the metrics counters."""
        self.api_calls = 0
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.estimated_cost = 0.0

    def get_and_reset_metrics(self) -> Dict[str, Any]:
        """Retrieves the current metrics and resets the counters."""
        metrics = {
            "api_calls": self.api_calls,
            "total_prompt_tokens": self.total_prompt_tokens,
            "total_completion_tokens": self.total_completion_tokens,
            "total_tokens": self.total_prompt_tokens + self.total_completion_tokens,
            "estimated_cost_cny": self.estimated_cost,
        }
        self.reset_metrics()
        return metrics

    def _update_metrics(self, usage: Dict[str, Any], model_name: str) -> Dict[str, Any]:
        """Updates metrics from the API response's usage object and returns per-call metrics."""
        if not usage:
            return {}

        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        # Get model-specific costs, default to generic if not found
        model_prices = self.model_token_prices_cny.get(model_name, self.model_token_prices_cny["default"])

        prompt_cost_cny_per_token = model_prices["input"] / 1000
        completion_cost_cny_per_token = model_prices["output"] / 1000

        cost_cny = (prompt_tokens * prompt_cost_cny_per_token) + (completion_tokens * completion_cost_cny_per_token)

        # Update cumulative instance metrics
        self.api_calls += 1
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        self.estimated_cost += cost_cny

        # Return metrics for this specific call
        return {"prompt_tokens": prompt_tokens, "completion_tokens": completion_tokens, "cost_cny": cost_cny}

    def _prepare_anthropic_payload(self, current_messages: list, model_name: str) -> tuple[dict, dict]:
        """
        ä¸ºAnthropic APIå‡†å¤‡è¯·æ±‚è½½è·ã€‚

        Returns:
            tuple: (headers, payload)
        """
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": self.anthropic_version,
            "content-type": "application/json",
        }

        system_message = ""
        other_messages = []
        for msg in current_messages:
            if msg["role"] == "system":
                system_message = msg["content"]
            else:
                other_messages.append(msg)

        if not other_messages:
            other_messages.append({"role": "user", "content": system_message})
            system_message = ""
        elif other_messages[0]["role"] != "user":
            other_messages.insert(0, {"role": "user", "content": "Continue."})

        payload = {
            "model": model_name,
            "max_tokens": 4096,
            "stream": False,
            "messages": other_messages,
        }
        if system_message:
            payload["system"] = system_message

        return headers, payload

    def _prepare_openai_payload(
        self, current_messages: list, model_name: str, temperature: float, role: str, expect_json: bool
    ) -> tuple[dict, dict]:
        """
        ä¸ºOpenAI APIå‡†å¤‡è¯·æ±‚è½½è·ã€‚

        Returns:
            tuple: (headers, payload)
        """
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model_name,
            "messages": current_messages,
            "temperature": temperature,
            "stream": False,
        }

        # å¦‚æœå¯ç”¨ extra_body å¹¶ä¸”ä¸ºè¯¥è§’è‰²é…ç½®äº†é off çš„æ€è€ƒæ¨¡å¼
        if LLM_EXTRA_BODY_ENABLED:
            thinking_mode = LLM_THINKING.get(role, LLM_THINKING.get("default", "off")).lower()
            if thinking_mode in ["hidden", "visible"]:
                payload["extra_body"] = {"thinking": thinking_mode}

        # å¼ºåˆ¶ JSON è¾“å‡º
        if expect_json:
            payload["response_format"] = {"type": "json_object"}

        return headers, payload

    async def _extract_response_content(self, api_response_json: dict, model_name: str) -> tuple[str, dict]:
        """
        ä»LLM APIå“åº”ä¸­æå–å†…å®¹å’ŒæŒ‡æ ‡ã€‚

        Returns:
            tuple: (content_string, call_metrics)
        """
        content_string = ""
        usage_data = api_response_json.get("usage")
        call_metrics = None

        if usage_data:
            call_metrics = self._update_metrics(usage_data, model_name)

        if self.provider == "anthropic":
            content_string = api_response_json["content"][0]["text"]
        else:  # OpenAI
            content_string = api_response_json["choices"][0]["message"]["content"]

        return content_string, call_metrics

    async def _handle_network_error(
        self, e: Exception, network_retries: int, max_retries: int
    ) -> tuple[bool, int]:
        """
        å¤„ç†ç½‘ç»œé”™è¯¯ã€‚

        Returns:
            tuple: (should_continue, new_retry_count)
        """
        network_retries += 1
        if network_retries > max_retries:
            self._get_console().print(f"[bold red]ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}[/bold red]")
            raise e

        wait_time = 5 * network_retries
        self._get_console().print(
            f"[bold yellow]ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œæ­£åœ¨ç­‰å¾… {wait_time} ç§’åé‡è¯•... ({network_retries}/{max_retries})[/bold yellow]"
        )
        await asyncio.sleep(wait_time)
        return True, network_retries

    async def _handle_rate_limit_error(
        self,
        e: Exception,
        current_api_key: str,
        fallback_used: bool,
        api_call_retries: int,
        max_retries: int,
    ) -> tuple[bool, str, bool, int]:
        """
        å¤„ç†APIé€Ÿç‡é™åˆ¶é”™è¯¯ã€‚

        Returns:
            tuple: (should_continue, new_api_key, new_fallback_used, new_retry_count)
        """
        # å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨key
        if not fallback_used and self.fallback_api_key is not None and self.fallback_api_key != "":
            current_api_key = self.fallback_api_key
            fallback_used = True
            self._get_console().print(
                "[bold yellow]429 error detected, using fallback API key for this request...[/bold yellow]"
            )
            return True, current_api_key, fallback_used, api_call_retries

        # TPMé™åˆ¶é‡è¯•
        if "tpm rate limit exceeded" in str(e):
            api_call_retries += 1
            if api_call_retries > max_retries:
                self._get_console().print("[bold red]API rate limit exceeded. Max retries reached.[/bold red]")
                raise e
            self._get_console().print(
                f"[bold yellow]API rate limit exceeded. Waiting 10 seconds to retry... ({api_call_retries}/{max_retries})[/bold yellow]"
            )
            await asyncio.sleep(10)
            return True, current_api_key, fallback_used, api_call_retries

        # å…¶ä»–429é”™è¯¯
        self._get_console().print(f"[bold red]API rate limit exceeded (not TPM): {e}[/bold red]")
        raise e

    async def send_message(
        self, messages: List[Dict[str, Any]], role: str = "default", expect_json: bool = True
    ) -> tuple[Dict | str | None, Dict | None]:
        """
        é€šè¿‡HTTP POSTæ–¹å¼å¼‚æ­¥å‘LLMå‘é€å¤šè½®æ¶ˆæ¯ï¼Œå¹¶è¿”å›è§£æåçš„å†…å®¹å’Œæœ¬æ¬¡è°ƒç”¨çš„æŒ‡æ ‡ã€‚
        å¢åŠ äº†JSONè§£æå¤±è´¥æ—¶çš„é‡è¯•é€»è¾‘ï¼Œå¹¶è¦æ±‚LLMä¿®æ­£æ ¼å¼ã€‚
        messages: [{"role": "system/user/assistant", "content": "..."}]
        role: è°ƒç”¨è€…çš„è§’è‰² (e.g., "planner", "executor")ï¼Œç”¨äºé€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
        Returns: A tuple containing:
                 - The parsed dictionary or raw string.
                 - A dictionary with the metrics for this specific call (tokens, cost).
                 Returns (None, None) if all retries fail.
        """
        model_name = self.models.get(role) or self.models.get("default")
        temperature = self.temperatures.get(role, self.temperatures.get("default", 0.2))

        json_parsing_retries = 0
        MAX_JSON_PARSE_RETRIES = 3  # å…è®¸2æ¬¡é‡è¯• (æ€»å…±3æ¬¡å°è¯•)

        api_call_retries = 0
        MAX_API_CALL_RETRIES = 10  # TPM limit retries
        network_retries = 0
        MAX_NETWORK_RETRIES = 3  # Network error retries

        current_messages = list(messages)  # å¤åˆ¶æ¶ˆæ¯åˆ—è¡¨ï¼Œä»¥ä¾¿åœ¨é‡è¯•æ—¶ä¿®æ”¹

        # ä¿®å¤ç‚¹ï¼šè®©å¯†é’¥é€‰æ‹©åœ¨æœ¬æ¬¡æ–¹æ³•çš„æ‰€æœ‰é‡è¯•ä¸­ä¿æŒçŠ¶æ€
        current_api_key = self.api_key
        fallback_used_in_this_request = False

        # å‘é€ LLM è¯·æ±‚äº‹ä»¶
        if broker and self.op_id:
            try:
                await broker.emit(
                    "llm.request",
                    {
                        "messages": current_messages,
                        "role": role,
                        "model": model_name,
                        "timestamp": asyncio.get_event_loop().time(),
                    },
                    op_id=self.op_id,
                )
            except Exception:
                pass

        while json_parsing_retries <= MAX_JSON_PARSE_RETRIES:
            try:
                # å‡†å¤‡APIè¯·æ±‚
                if self.provider == "anthropic":
                    headers, payload = self._prepare_anthropic_payload(current_messages, model_name)
                else:  # OpenAI
                    headers, payload = self._prepare_openai_payload(
                        current_messages, model_name, temperature, role, expect_json
                    )

                # æ›´æ–°headersä¸­çš„API key
                if self.provider == "anthropic":
                    headers["x-api-key"] = current_api_key
                else:
                    headers["Authorization"] = f"Bearer {current_api_key}"

                # å‘é€è¯·æ±‚
                response = await self.client.post(self.api_url, headers=headers, json=payload, timeout=1200.0)

                if response.status_code != 200:
                    raise Exception(f"LLM APIè¯·æ±‚å¤±è´¥: {response.status_code} {response.text}")

                # æå–å“åº”å†…å®¹
                api_response_json = json.loads(response.text)
                content_string, call_metrics = await self._extract_response_content(api_response_json, model_name)

                # å‘é€å“åº”äº‹ä»¶
                if broker and self.op_id:
                    try:
                        await broker.emit(
                            "llm.response",
                            {
                                "content": content_string,
                                "role": role,
                                "model": model_name,
                                "metrics": call_metrics,
                                "timestamp": asyncio.get_event_loop().time(),
                            },
                            op_id=self.op_id,
                        )
                    except Exception:
                        pass

                # å¤„ç†JSONè§£æ
                if expect_json:
                    final_json = self._robust_json_parser(content_string)
                    if final_json is not None:
                        return final_json, call_metrics
                    else:
                        raise json.JSONDecodeError("LLM content is not valid JSON.", content_string, 0)
                else:
                    return content_string, call_metrics

            except (httpx.ConnectError, httpx.TimeoutException, httpx.NetworkError) as e:
                should_continue, network_retries = await self._handle_network_error(e, network_retries, MAX_NETWORK_RETRIES)
                if should_continue:
                    continue

            except json.JSONDecodeError:
                json_parsing_retries += 1
                if json_parsing_retries > MAX_JSON_PARSE_RETRIES:
                    self._get_console().print("[bold red]JSONè§£ææœ€ç»ˆå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚[/bold red]")
                    return None, None

                self._get_console().print(
                    f"[bold yellow]è­¦å‘Šï¼šLLMè¿”å›çš„JSONæ ¼å¼æ— æ•ˆï¼Œæ­£åœ¨å°è¯•ç¬¬ {json_parsing_retries}/{MAX_JSON_PARSE_RETRIES} æ¬¡é‡è¯•...[/bold yellow]"
                )
                current_messages.append(
                    {
                        "role": "user",
                        "content": "Your previous response was not valid JSON. Please correct the format and provide the full response again, ensuring it is a single, valid JSON object.",
                    }
                )
                await asyncio.sleep(1)

            except Exception as e:
                if "429" in str(e):
                    should_continue, current_api_key, fallback_used_in_this_request, api_call_retries = (
                        await self._handle_rate_limit_error(
                            e, current_api_key, fallback_used_in_this_request, api_call_retries, MAX_API_CALL_RETRIES
                        )
                    )
                    if should_continue:
                        continue
                elif "tpm rate limit exceeded" in str(e):
                    api_call_retries += 1
                    if api_call_retries > MAX_API_CALL_RETRIES:
                        self.console.print("[bold red]API rate limit exceeded. Max retries reached.[/bold red]")
                        raise e
                    self.console.print(
                        f"[bold yellow]API rate limit exceeded. Waiting 10 seconds to retry... ({api_call_retries}/{MAX_API_CALL_RETRIES})[/bold yellow]"
                    )
                    await asyncio.sleep(10)
                    continue

                self._get_console().print(f"[bold red]å‘é€æ¶ˆæ¯æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}[/bold red]")
                raise e
        return None, None

    def _generate_preservation_aware_compression_prompt(self, history_to_compress: List[Dict[str, str]]) -> str:
        """
        ç”Ÿæˆä¿æŠ¤æ¢ç´¢æ€§æ€ç»´çš„å‹ç¼©æç¤ºè¯ï¼Œç”¨äºLLMæ€»ç»“å¯¹è¯å†å²ã€‚
        Args:
            history_to_compress: è¦å‹ç¼©çš„å¯¹è¯å†å²ç‰‡æ®µã€‚
        Returns:
            ç”¨äºå‹ç¼©çš„ç³»ç»Ÿæç¤ºè¯ã€‚
        """
        history_content = ""
        for i, msg in enumerate(history_to_compress):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            history_content += f"\n[æ¶ˆæ¯{i + 1}] {role}:\n{content}\n" + "-" * 50

        return f"""ä½ æ˜¯LuaN1aoæ¸—é€æµ‹è¯•æ™ºèƒ½ä½“çš„è®°å¿†ç®¡ç†ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸ºç®€æ´çš„è‡ªç„¶è¯­è¨€æ‘˜è¦ï¼Œç”¨äºåç»­æµ‹è¯•å†³ç­–å‚è€ƒã€‚

## å‹ç¼©è¦æ±‚

### ğŸ“‹ å¿…é¡»ä¿ç•™çš„å…³é”®ä¿¡æ¯
1. **å®‰å…¨å‘ç°** - æ‰€æœ‰æ¼æ´ã€å¼‚å¸¸å“åº”ã€é”™è¯¯ä¿¡æ¯å¿…é¡»è¯¦ç»†è®°å½•
2. **æŠ€æœ¯ç»†èŠ‚** - ç›®æ ‡ç³»ç»Ÿçš„æŠ€æœ¯æ ˆã€ç‰ˆæœ¬ã€é…ç½®ä¿¡æ¯
3. **æ”»å‡»é¢** - å·²å‘ç°çš„è¾“å…¥ç‚¹ã€å‚æ•°ã€ç«¯ç‚¹
4. **æµ‹è¯•è¿›å±•** - å½“å‰æµ‹è¯•é˜¶æ®µã€å·²å®Œæˆçš„æµ‹è¯•ç±»å‹
5. **æœ‰æ•ˆè½½è·** - æˆåŠŸæˆ–æœ‰ä»·å€¼çš„æ”»å‡»payload
6. **æ¢ç´¢æ€§æ€ç»´** - åŒ…å«"è¯•è¯•"ã€"å¯èƒ½"ã€"ç®€å•"ã€"ç›´æ¥"ç­‰æ¢ç´¢æ€§è¡¨è¿°çš„æ€ç»´è¿‡ç¨‹
7. **ç›´è§‰åˆ¤æ–­** - åŸºäºç»éªŒçš„å¿«é€Ÿåˆ¤æ–­å’Œå‡è®¾
8. **ç­–ç•¥è½¬æ¢èŠ‚ç‚¹** - å†³å®šæ”¹å˜æµ‹è¯•æ–¹å‘çš„å…³é”®æ€è€ƒ

### ğŸ”„ å¯ä»¥é€‚åº¦å‹ç¼©çš„å†…å®¹
1. **é‡å¤æ“ä½œ** - å¤šæ¬¡ç›¸åŒçš„å·¥å…·è°ƒç”¨ï¼ˆä¿ç•™ç»“æœå³å¯ï¼‰
2. **å†—é•¿çš„ç†è®ºåˆ†æ** - ä¿ç•™æ ¸å¿ƒç»“è®º
3. **æ­£å¸¸çš„ä¸šåŠ¡åŠŸèƒ½éªŒè¯** - ç®€åŒ–æè¿°è¿‡ç¨‹

### ğŸ§  ç‰¹åˆ«ä¿æŠ¤åŸåˆ™
- **å¿…é¡»ä¿ç•™åŸå§‹Payloadå­—ç¬¦ä¸²**ï¼šå³ä½¿å¤±è´¥ï¼Œä¹Ÿè¦ä¿ç•™å¦‚ `'--`ã€`OR 1=1`ã€`SLEEP(5)` ç­‰å…·ä½“Payloadã€‚
- ä¿æŒæµ‹è¯•æ€ç»´çš„"ç«èŠ±"å’Œ"ç›´è§‰"
- ä¸è¦è¿‡åº¦ç†æ€§åŒ–ï¼Œä¿ç•™ä¸€äº›çœ‹ä¼¼"ä¸æˆç†Ÿ"ä½†å¯èƒ½æœ‰ä»·å€¼çš„æƒ³æ³•
- ç‰¹åˆ«ä¿æŠ¤é‚£äº›æåˆ°"ç®€å•æ–¹æ³•"ã€"åŸºç¡€ç»•è¿‡"çš„å†…å®¹
- ä¿ç•™æ¢ç´¢æ€§çš„"ä¹Ÿè®¸åº”è¯¥è¯•è¯•..."ç±»å‹çš„æ€è€ƒ

## ğŸ“ è¾“å‡ºæ ¼å¼

è¯·ç”¨è‡ªç„¶è¯­è¨€å†™æˆä¸€ä¸ªè¿è´¯çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

**æµ‹è¯•ç›®æ ‡ä¸è¿›å±•**
- å½“å‰æµ‹è¯•çš„ç›®æ ‡ç³»ç»Ÿæ¦‚å†µ
- æµ‹è¯•è¿›å±•åˆ°çš„é˜¶æ®µï¼ˆå‘ç°/æšä¸¾/æ¼æ´æµ‹è¯•/åˆ©ç”¨ï¼‰

**å…³é”®å®‰å…¨å‘ç°**
- æŒ‰é‡è¦æ€§æ’åºçš„å®‰å…¨å‘ç°
- æ¯ä¸ªå‘ç°åŒ…æ‹¬ï¼šæè¿°ã€è¯æ®ã€æ½œåœ¨å½±å“

**æŠ€æœ¯ç¯å¢ƒåˆ†æ**
- ç›®æ ‡ç³»ç»Ÿçš„æŠ€æœ¯æ ˆå’Œæ¶æ„
- å·²è¯†åˆ«çš„æ”»å‡»é¢å’Œå…¥å£ç‚¹

**æµ‹è¯•ç­–ç•¥ä¸è°ƒæ•´**
- å·²å°è¯•çš„æµ‹è¯•æ–¹æ³•åŠæ•ˆæœ
- æ ¹æ®å‘ç°è°ƒæ•´çš„æµ‹è¯•ç­–ç•¥

**åç»­å»ºè®®**
- åŸºäºå½“å‰å‘ç°çš„åç»­æµ‹è¯•æ–¹å‘
- éœ€è¦é‡ç‚¹å…³æ³¨çš„æ½œåœ¨é£é™©ç‚¹

## ğŸ“Š å¾…å‹ç¼©çš„å¯¹è¯å†å²ï¼š

{history_content}

è¯·å°†ä¸Šè¿°å†å²å‹ç¼©ä¸ºä¸€ä»½ç®€æ´ä½†å®Œæ•´çš„æµ‹è¯•è¿›å±•æŠ¥å‘Šï¼Œç¡®ä¿æ‰€æœ‰å…³é”®å®‰å…¨ä¿¡æ¯å’Œæ¢ç´¢æ€§æ€ç»´éƒ½å¾—åˆ°å¦¥å–„ä¿ç•™ã€‚æŠ¥å‘Šåº”è¯¥è®©å¦ä¸€ä¸ªæ™ºèƒ½ä½“èƒ½å¤ŸåŸºäºè¿™ä»½æ‘˜è¦ç»§ç»­è¿›è¡Œæœ‰æ•ˆçš„æ¸—é€æµ‹è¯•ã€‚"""

    async def summarize_conversation(self, messages_to_summarize: List[Dict[str, str]]) -> str | None:
        """
        ä½¿ç”¨LLMæ€»ç»“ä¸€æ®µå¯¹è¯ï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œå¹¶éµå¾ªä¿æŠ¤æ¢ç´¢æ€§æ€ç»´çš„åŸåˆ™ã€‚
        Args:
            messages_to_summarize: è¦æ€»ç»“çš„å¯¹è¯å†å²ç‰‡æ®µã€‚
        Returns:
            å¯¹è¯çš„ç®€æ´æ‘˜è¦ã€‚
        """
        compression_prompt_content = self._generate_preservation_aware_compression_prompt(messages_to_summarize)

        summarization_messages = [{"role": "user", "content": compression_prompt_content}]
        # Use a specific role for summarization to potentially use a different model/temperature
        summary = await self.send_message(summarization_messages, role="summarizer", expect_json=False)
        return summary

    def _clean_json_string(self, json_string: str) -> str:
        """
        æ¸…ç†JSONå­—ç¬¦ä¸²ï¼šç§»é™¤BOMã€ç©ºç™½ã€Markdownä»£ç å—ã€‚

        Returns:
            æ¸…ç†åçš„å­—ç¬¦ä¸²
        """
        # ç§»é™¤ UTF-8 BOM
        if json_string.startswith("\ufeff"):
            json_string = json_string.lstrip("\ufeff")

        json_string = json_string.strip()

        # å¤„ç† Markdown å›´æ ä»£ç å—
        if json_string.startswith("```json"):
            json_string = json_string[7:]
        if json_string.startswith("```"):
            json_string = json_string[3:]
        if json_string.startswith("~~~"):
            json_string = json_string[3:]

        if json_string.endswith("```"):
            json_string = json_string[:-3]
        if json_string.endswith("~~~"):
            json_string = json_string[:-3]

        return json_string.strip()

    def _try_parse_json(self, json_string: str) -> Dict | None:
        """
        å°è¯•è§£æJSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯æ•°ç»„åˆ™åŒ…è£…ä¸ºå¯¹è±¡ã€‚

        Returns:
            è§£æçš„å­—å…¸æˆ–None
        """
        try:
            parsed = json.loads(json_string)
            if isinstance(parsed, list):
                return {"list": parsed}
            if isinstance(parsed, dict):
                return parsed
        except Exception:
            pass
        return None

    def _extract_json_from_text(self, json_string: str) -> str | None:
        """
        ä»æ–‡æœ¬ä¸­æå–JSONæ®µè½ã€‚

        Returns:
            æå–çš„JSONå­—ç¬¦ä¸²æˆ–None
        """
        try:
            brace_start = json_string.index("{") if "{" in json_string else None
            bracket_start = json_string.index("[") if "[" in json_string else None

            candidates = [i for i in [brace_start, bracket_start] if i is not None]
            if not candidates:
                self._get_console().print(Panel("JSONè§£æå™¨åœ¨å­—ç¬¦ä¸²ä¸­æœªæ‰¾åˆ° '{' æˆ– '['ã€‚", title="è­¦å‘Š", style="yellow"))
                return None
            start_index = min(candidates)

            brace_end = json_string.rindex("}") if "}" in json_string else -1
            bracket_end = json_string.rindex("]") if "]" in json_string else -1
            end_index = max(brace_end, bracket_end)

            if end_index < start_index:
                self._get_console().print(Panel("JSONè§£æå™¨æœªèƒ½å®šä½åŒ¹é…çš„ç»“æŸç¬¦ '}' æˆ– ']'ã€‚", title="è­¦å‘Š", style="yellow"))
                return None

            return json_string[start_index : end_index + 1].strip()
        except ValueError:
            self._get_console().print(Panel("JSONè§£æå™¨åœ¨å­—ç¬¦ä¸²ä¸­æœªæ‰¾åˆ° '{}' æˆ– '}'ã€‚", title="è­¦å‘Š", style="yellow"))
            return None

    def _apply_soft_fixes(self, json_str: str) -> str:
        """
        å¯¹JSONå­—ç¬¦ä¸²åº”ç”¨è½»åº¦çº é”™ï¼šæ›¿æ¢Pythonå­—é¢é‡ã€ç§»é™¤å°¾éšé€—å·ã€å¤„ç†å•å¼•å·ã€‚

        Returns:
            ä¿®å¤åçš„å­—ç¬¦ä¸²
        """
        fixed = json_str
        # å°† Python å¸ƒå°”/ç©ºå€¼æ›¿æ¢ä¸º JSON
        fixed = fixed.replace(" None", " null").replace(": None", ": null")
        fixed = fixed.replace(" True", " true").replace(": True", ": true")
        fixed = fixed.replace(" False", " false").replace(": False", ": false")
        # ç®€å•å¤„ç†å°¾éšé€—å·
        fixed = re.sub(r",\s*}\s*$", "}", fixed)
        fixed = re.sub(r",\s*]\s*$", "]", fixed)
        # å¤„ç†å•å¼•å·
        if fixed.count('"') == 0 and fixed.count("'") >= 2:
            fixed = fixed.replace("'", '"')
        return fixed

    def _robust_json_parser(self, json_string: str) -> Dict | None:
        """æ›´å¥å£®çš„ JSON è§£æå™¨ï¼š
        - å¤„ç† UTF-8 BOMã€é¦–å°¾ç©ºç™½
        - æ¸…ç† Markdown ä»£ç å—ï¼ˆ```jsonã€```ã€~~~ ç­‰ï¼‰
        - ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªå®Œæ•´ JSON å¯¹è±¡æˆ–æ•°ç»„
        - å®¹å¿æ ¹ä¸ºæ•°ç»„çš„åˆæ³•è¾“å‡º
        - å¯¹å¸¸è§é”™è¯¯è¿›è¡Œè½»åº¦çº æ­£ï¼ˆå°¾éšé€—å·ã€å•å¼•å·ã€None/True/Falseç­‰ Python å­—é¢é‡ï¼‰
        """
        if not isinstance(json_string, str):
            return None

        # 1. æ¸…ç†å­—ç¬¦ä¸²
        json_string = self._clean_json_string(json_string)
        if not json_string:
            return None

        # 2. å°è¯•ç›´æ¥è§£æ
        result = self._try_parse_json(json_string)
        if result is not None:
            return result

        # 3. ä»æ–‡æœ¬ä¸­æå–JSON
        candidate = self._extract_json_from_text(json_string)
        if candidate is None:
            return None

        # 4. åº”ç”¨è½»åº¦çº é”™
        candidate_fixed = self._apply_soft_fixes(candidate)

        # 5. å°è¯•è§£æä¿®å¤Šåçš„å­—ç¬¦ä¸²
        try:
            parsed = json.loads(candidate_fixed)
            if isinstance(parsed, list):
                return {"list": parsed}
            if isinstance(parsed, dict):
                return parsed
            return None
        except json.JSONDecodeError as e:
            self._get_console().print(
                Panel(f"JSONè§£æå¤±è´¥: {e}\nåŸå§‹å­—ç¬¦ä¸² (æ¸…ç†å): {candidate_fixed[:500]}...", title="è­¦å‘Š", style="yellow")
            )
            return None
        except Exception:
            return None


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    import asyncio
    import sys

    async def main():
        provider = "openai"
        if len(sys.argv) > 1 and sys.argv[1] == "anthropic":
            provider = "anthropic"

        # Manually override the provider for testing
        from conf import config

        config.LLM_PROVIDER = provider

        client = LLMClient()
        msgs = [{"role": "user", "content": "hi"}]
        # Test using the specified provider
        print(f"Sending message with provider: {provider}")
        reply = await client.send_message(msgs)
        print("Model reply:", reply)

    asyncio.run(main())
